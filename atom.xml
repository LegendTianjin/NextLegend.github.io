<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Next Legend!</title>
  
  <subtitle>一天进步一点点</subtitle>
  <link href="/NextLegend.github.io/atom.xml" rel="self"/>
  
  <link href="https://legendtianjin.github.io/NextLegend.github.io/"/>
  <updated>2018-12-15T04:44:15.170Z</updated>
  <id>https://legendtianjin.github.io/NextLegend.github.io/</id>
  
  <author>
    <name>赵小亮</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>最近的一些思考</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/12/15/%E6%9C%80%E8%BF%91%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/12/15/最近的一些思考/</id>
    <published>2018-12-15T04:36:54.000Z</published>
    <updated>2018-12-15T04:44:15.170Z</updated>
    
    <content type="html"><![CDATA[<p><center><font color="black" size="6"><strong>短暂停留与思考</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>今天是2018年12月15号，距离小亮上次发文已经过去两个月了，这两个月虽然中间有一次研究生数学考试，但是更多的是时间，而小亮却没有坚持继续与大家分享博客，这是小亮的不足，在此深表歉意，为此立下军令状，以后两天更新一篇博文，等到短暂的研究生职业生涯结束时，那时候再回头看看，这又何不是一笔宝贵的财富呢？ 我是赵小亮，一枚NLP大道上的小白，喜欢用文字记载生活、学习、科研中的事情，因为我觉得文字是有温度的，可以暖化人心，在冰冷枯燥的科研之路上，你我的互相借力，又何尝不是一种结果呢？This is for you! I promise!<br>&#160; &#160; &#160; &#160;AI大餐预告：下周小亮会出一个Attention系列专题，大概有六大板块，从Attention Mechanism的起源、历史、变种、相关论文《Attention Is All You Need》以及实战，还有一些思考，带领大家一起吃Attention这顿大餐！</strong><br>一、生活是具体的<br>&#160; &#160; &#160; &#160;小亮最近虽然没有写技术博客，停滞了两个月，内心无比自责，愧对于各位AI技术爱好者。但是，最近倒是读了一些书籍，不管是不是所谓的心灵鸡汤，我觉得凡事能够促进你积极前进的在这一阶段都是良药，下面小亮就和各位share一些心得体会，愿在这冬日的寒风凛冽中，给予一丝温暖。<br>&#160; &#160; &#160; &#160;是这样的，小亮最近读的这本书叫《具体生活》，这本书一共有七章，我们今天来分享前三章，后面的小亮会随后附上，这本书是由清华大学吴军老师撰写的（各位如果不知道吴军老师的话，可以百度一下哈！小亮这里给出网址 <a href="https://baike.baidu.com/item/吴军/8125425?fr=aladdin）下面是小亮在读书的时候觉得书中内容写的触动心灵的部分，与君共品之！" target="_blank" rel="noopener">https://baike.baidu.com/item/吴军/8125425?fr=aladdin）下面是小亮在读书的时候觉得书中内容写的触动心灵的部分，与君共品之！</a></p><h3 id="一、生活是具体的"><a href="#一、生活是具体的" class="headerlink" title="一、生活是具体的"></a>一、生活是具体的</h3><p>&#160; &#160; &#160; &#160; 小亮最近虽然没有写技术博客，停滞了两个月，内心无比自责，愧对于各位AI技术爱好者。但是，最近倒是读了一些书籍，不管是不是所谓的心灵鸡汤，我觉得凡事能够促进你积极前进的在这一阶段都是良药，下面小亮就和各位share一些心得体会，愿在这冬日的寒风凛冽中，给予一丝温暖。<br>&#160; &#160; &#160; &#160; <strong>是这样的，小亮最近读的这本书叫《具体生活》，这本书一共有七章，我们今天来分享前三章，后面的小亮会随后附上，这本书是由清华大学吴军老师撰写的（各位如果不知道吴军老师的话，可以百度一下哈！小亮这里给出网址 <a href="https://baike.baidu.com/item/吴军/8125425?fr=aladdin）" target="_blank" rel="noopener">https://baike.baidu.com/item/吴军/8125425?fr=aladdin）</a></strong> <strong>下面是小亮在读书的时候觉得书中内容写的触动心灵的部分，与君共品之！</strong><br><img src="/NextLegend.github.io/2018/12/15/最近的一些思考/006.jpg" alt="最近的一些思考"></p><h4 id="1-具体生活-不是抽象任性的美好畅想"><a href="#1-具体生活-不是抽象任性的美好畅想" class="headerlink" title="1.  具体生活 不是抽象任性的美好畅想"></a>1. <font color="red"> <strong>具体生活 不是抽象任性的美好畅想</strong></font></h4><p> &#160; &#160; &#160; &#160; 人是矛盾的动物。大部分人会说，工作是为了更好地生活，但实际上，人常会在工作和生活中产生矛盾时，选择放弃生活，而忘记了工作的初衷。每个人的具体生活，都是独一无二的，既不能由别人代替，也不可能在以后的时间补上。<strong>生活的风范、品味和认知水平或许能够帮助你，找回真实的自己。</strong></p><h4 id="2-第一章：旅行的意义"><a href="#2-第一章：旅行的意义" class="headerlink" title="2.  第一章：旅行的意义"></a>2. <font color="purple"> <strong>第一章：旅行的意义</strong></font></h4><p> &#160; &#160; &#160; &#160; <strong>为什么要旅行？虽然不同的人有不同的看法，但是人们通常会说“行万里路胜于读万卷书。<font color="purple"> 平心而论，很难讲读万卷书和行万里路哪一个收获更大。</font>不过，行万里路的收获常常是读书、看电视，或者上网学习所得不到的。因此我常常认为读书和旅行的收益是互补的。</strong></p><h4 id="3-第二章：博物馆之美"><a href="#3-第二章：博物馆之美" class="headerlink" title="3. 第二章：博物馆之美"></a>3.<font color="Orange"> <strong>第二章：博物馆之美</strong></font></h4><p> &#160; &#160; &#160; &#160; <strong><font color="Orange"> 每到一个新的城市，我都会去当地最具有特色的博物馆参观，因为博物馆通常浓缩了一个地方的历史和文化。</font>各地不同的博物馆看得多了，不仅可以体会不同地域人类的文明和生存方式，了解世界各地文化的多样性，而且慢慢地就能够绘制出人类文明的全图。在这一章里面，我们一起来看看在世界上那些著名的博物馆里，有哪些人类的文明足迹和艺术成就。</strong></p><h4 id="4-第三章：读书以怡情长智"><a href="#4-第三章：读书以怡情长智" class="headerlink" title="4.  第三章：读书以怡情长智"></a>4. <font color="blue"> <strong>第三章：读书以怡情长智</strong></font></h4><p> &#160; &#160; &#160; &#160; <strong>古人把读万卷书和行万里路看作精英阶层成长不可或缺的两个环节，它们既能使人获取知识、也能令人愉悦自我。今天因为互联网或其他展示形式更丰富的媒体，使得获取知识似乎变得更容易了，那么读书，特别是读纸质书是否还有必要呢？<font color="blue"> 答案是肯定的，因为读书的好处远不止获取知识和愉悦自我，它还是我们与世界交流的一种方式，也是我们思想形成的一个环节。</font></strong></p><h3 id="二、修炼内功"><a href="#二、修炼内功" class="headerlink" title="二、修炼内功"></a>二、修炼内功</h3><h4 id="160-160-160-160-“内功”这个词小亮第一次听是跟着导师，第二次是在这里——-gt-《任正非内部讲话曝光：美国不认同我们，我们就把5G做得更好，争取更多的西方客户》-以下内容转载自创财经，非商业等其他用途。"><a href="#160-160-160-160-“内功”这个词小亮第一次听是跟着导师，第二次是在这里——-gt-《任正非内部讲话曝光：美国不认同我们，我们就把5G做得更好，争取更多的西方客户》-以下内容转载自创财经，非商业等其他用途。" class="headerlink" title="&#160; &#160; &#160; &#160; “内功”这个词小亮第一次听是跟着导师，第二次是在这里——&gt;《任正非内部讲话曝光：美国不认同我们，我们就把5G做得更好，争取更多的西方客户》(以下内容转载自创财经，非商业等其他用途。)"></a>&#160; &#160; &#160; &#160; <strong>“内功”这个词小亮第一次听是跟着导师，第二次是在这里——&gt;<font color="red">《任正非内部讲话曝光：美国不认同我们，我们就把5G做得更好，争取更多的西方客户》</font></strong>(以下内容转载自<a href="https://baijiahao.baidu.com/s?id=1619737135282467009&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">创财经</a>，非商业等其他用途。)</h4><h4 id="160-160-160-160-近日，任正非一篇最新内部讲话曝光，讲话中理性分析了中国和西方价值观的不同，并表示，只有站在西方的观念上理解西方，才能进得去西方国家。要从人类文明的结晶中，去寻找解决世界问题的钥匙。任正非称，华为员工要多练内功，内功的强大才是真正的强大，抗住外部压力要靠内功。现在社会过分夸大了华为，这是有害的，特别是别让我们的年轻人，以为公司真的成功了，而麻痹起来。华为要将外部环境的压力变成倒逼我们业务创新与管理改进的动力。同时我们不能低估全球权力格局的动态变化，不能盲目自信，那就像100多年前义和团那样了。尽管我们不被个别西方国家认同，不要埋怨，因为我们做得还不够好。美国不认同我们，我们就把5G做得更好，争取更多的西方客户。"><a href="#160-160-160-160-近日，任正非一篇最新内部讲话曝光，讲话中理性分析了中国和西方价值观的不同，并表示，只有站在西方的观念上理解西方，才能进得去西方国家。要从人类文明的结晶中，去寻找解决世界问题的钥匙。任正非称，华为员工要多练内功，内功的强大才是真正的强大，抗住外部压力要靠内功。现在社会过分夸大了华为，这是有害的，特别是别让我们的年轻人，以为公司真的成功了，而麻痹起来。华为要将外部环境的压力变成倒逼我们业务创新与管理改进的动力。同时我们不能低估全球权力格局的动态变化，不能盲目自信，那就像100多年前义和团那样了。尽管我们不被个别西方国家认同，不要埋怨，因为我们做得还不够好。美国不认同我们，我们就把5G做得更好，争取更多的西方客户。" class="headerlink" title="&#160; &#160; &#160; &#160; 近日，任正非一篇最新内部讲话曝光，讲话中理性分析了中国和西方价值观的不同，并表示，只有站在西方的观念上理解西方，才能进得去西方国家。要从人类文明的结晶中，去寻找解决世界问题的钥匙。任正非称，华为员工要多练内功，内功的强大才是真正的强大，抗住外部压力要靠内功。现在社会过分夸大了华为，这是有害的，特别是别让我们的年轻人，以为公司真的成功了，而麻痹起来。华为要将外部环境的压力变成倒逼我们业务创新与管理改进的动力。同时我们不能低估全球权力格局的动态变化，不能盲目自信，那就像100多年前义和团那样了。尽管我们不被个别西方国家认同，不要埋怨，因为我们做得还不够好。美国不认同我们，我们就把5G做得更好，争取更多的西方客户。"></a>&#160; &#160; &#160; &#160; <strong>近日，任正非一篇最新内部讲话曝光，讲话中理性分析了中国和西方价值观的不同，并表示，只有站在西方的观念上理解西方，才能进得去西方国家。要从人类文明的结晶中，去寻找解决世界问题的钥匙。任正非称，华为员工要多练内功，内功的强大才是真正的强大，抗住外部压力要靠内功。现在社会过分夸大了华为，这是有害的，特别是别让我们的年轻人，以为公司真的成功了，而麻痹起来。华为要将外部环境的压力变成倒逼我们业务创新与管理改进的动力。同时我们不能低估全球权力格局的动态变化，不能盲目自信，那就像100多年前义和团那样了。尽管我们不被个别西方国家认同，不要埋怨，因为我们做得还不够好。美国不认同我们，我们就把5G做得更好，争取更多的西方客户。</strong></h4><p><img src="/NextLegend.github.io/2018/12/15/最近的一些思考/008.png" alt="最近的一些思考"></p><h3 id="160-160-160-160-从人类文明的结晶中，找到解决世界问题的钥匙——任正非在公共关系战略纲要汇报会上的讲话-2018年9月29日"><a href="#160-160-160-160-从人类文明的结晶中，找到解决世界问题的钥匙——任正非在公共关系战略纲要汇报会上的讲话-2018年9月29日" class="headerlink" title="&#160; &#160; &#160; &#160;从人类文明的结晶中，找到解决世界问题的钥匙——任正非在公共关系战略纲要汇报会上的讲话_2018年9月29日"></a>&#160; &#160; &#160; &#160;从人类文明的结晶中，找到解决世界问题的钥匙——任正非在公共关系战略纲要汇报会上的讲话_2018年9月29日</h3><h3 id="160-160-160-160-一、我们要解决在西方遇到的问题，首先要充分认识西方的价值观，站在他们的立场去理解他们。"><a href="#160-160-160-160-一、我们要解决在西方遇到的问题，首先要充分认识西方的价值观，站在他们的立场去理解他们。" class="headerlink" title="&#160; &#160; &#160; &#160;一、我们要解决在西方遇到的问题，首先要充分认识西方的价值观，站在他们的立场去理解他们。"></a>&#160; &#160; &#160; &#160;一、我们要解决在西方遇到的问题，首先要充分认识西方的价值观，站在他们的立场去理解他们。</h3><h3 id="160-160-160-160-二、学点哲学、历史、社会学、心理学、国际法律秩序及权力分配学……-，从中找到解决世界问题的钥匙。公共关系纲要中，哲学、历史、社会学和心理学等都需要放进来，这些人类文明的结晶，会带着我们找到解决世界问题的钥匙。"><a href="#160-160-160-160-二、学点哲学、历史、社会学、心理学、国际法律秩序及权力分配学……-，从中找到解决世界问题的钥匙。公共关系纲要中，哲学、历史、社会学和心理学等都需要放进来，这些人类文明的结晶，会带着我们找到解决世界问题的钥匙。" class="headerlink" title="&#160; &#160; &#160; &#160;二、学点哲学、历史、社会学、心理学、国际法律秩序及权力分配学…….，从中找到解决世界问题的钥匙。公共关系纲要中，哲学、历史、社会学和心理学等都需要放进来，这些人类文明的结晶，会带着我们找到解决世界问题的钥匙。"></a>&#160; &#160; &#160; &#160;二、学点哲学、历史、社会学、心理学、国际法律秩序及权力分配学…….，从中找到解决世界问题的钥匙。公共关系纲要中，哲学、历史、社会学和心理学等都需要放进来，这些人类文明的结晶，会带着我们找到解决世界问题的钥匙。</h3><h3 id="160-160-160-160-三、基础研究突破正在结构性深化，我们还没有被产业认同，是因为我们做得还不够好。"><a href="#160-160-160-160-三、基础研究突破正在结构性深化，我们还没有被产业认同，是因为我们做得还不够好。" class="headerlink" title="&#160; &#160; &#160; &#160;三、基础研究突破正在结构性深化，我们还没有被产业认同，是因为我们做得还不够好。"></a>&#160; &#160; &#160; &#160;三、基础研究突破正在结构性深化，我们还没有被产业认同，是因为我们做得还不够好。</h3><h3 id="160-160-160-160-四、未来公共关系的价值观与故略纲领是“合作共赢”，要建立一个开放的思想架构。你们是一把伞，可能与业务部门有冲突，各说各的调，唱唱双簧，他们做他们的“矛”，也没有什么不好，没有必要步调一致。合作共赢是公司的大思想，实现过程是困难的，要允许部门不听话，慢慢会转过来的，这就是华为。"><a href="#160-160-160-160-四、未来公共关系的价值观与故略纲领是“合作共赢”，要建立一个开放的思想架构。你们是一把伞，可能与业务部门有冲突，各说各的调，唱唱双簧，他们做他们的“矛”，也没有什么不好，没有必要步调一致。合作共赢是公司的大思想，实现过程是困难的，要允许部门不听话，慢慢会转过来的，这就是华为。" class="headerlink" title="&#160; &#160; &#160; &#160;四、未来公共关系的价值观与故略纲领是“合作共赢”，要建立一个开放的思想架构。你们是一把伞，可能与业务部门有冲突，各说各的调，唱唱双簧，他们做他们的“矛”，也没有什么不好，没有必要步调一致。合作共赢是公司的大思想，实现过程是困难的，要允许部门不听话，慢慢会转过来的，这就是华为。"></a>&#160; &#160; &#160; &#160;四、未来公共关系的价值观与故略纲领是“合作共赢”，要建立一个开放的思想架构。你们是一把伞，可能与业务部门有冲突，各说各的调，唱唱双簧，他们做他们的“矛”，也没有什么不好，没有必要步调一致。合作共赢是公司的大思想，实现过程是困难的，要允许部门不听话，慢慢会转过来的，这就是华为。</h3><h3 id="160-160-160-160-五、公共关系要从一个部门，走向一个场态。"><a href="#160-160-160-160-五、公共关系要从一个部门，走向一个场态。" class="headerlink" title="&#160; &#160; &#160; &#160;五、公共关系要从一个部门，走向一个场态。"></a>&#160; &#160; &#160; &#160;五、公共关系要从一个部门，走向一个场态。</h3>]]></content>
    
    <summary type="html">
    
      今天是2018年12月15号，距离小亮上次发文已经过去两个月了，这两个月虽然中间有一次研究生数学考试，但是更多的是时间，而小亮却没有坚持继续与大家分享博客，这是小亮的不足，在此深表歉意，为此立下军令状，以后两天更新一篇博文，等到短暂的研究生职业生涯结束时，那时候再回头看看，这又何不是一笔宝贵的财富呢？ 我是赵小亮，一枚NLP大道上的小白，喜欢用文字记载生活、学习、科研中的事情，因为我觉得文字是有温度的，可以暖化人心，在冰冷枯燥的科研之路上，你我的互相借力，又何尝不是一种结果呢？This is for you! I promise!
    
    </summary>
    
    
      <category term="具体生活" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E5%85%B7%E4%BD%93%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title>机器学习Day6</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Day6/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/10/07/机器学习Day6/</id>
    <published>2018-10-07T02:52:26.000Z</published>
    <updated>2018-10-07T03:40:06.527Z</updated>
    
    <content type="html"><![CDATA[<p><center><font color="black" size="6"><strong>Day6逻辑回归之代码实战</strong></font></center></p><p><center><font color="black" size="4"><strong>一、温习之</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>上来再把这张逻辑回归的图放在这里，让大家再温习一下逻辑回归的基本概念哈！</strong><br><img src="/NextLegend.github.io/2018/10/07/机器学习Day6/Day6.jpg" alt="机器学习Day6"></p><p><center><font color="black" size="4"><strong>二、介绍之</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>好，下面开始进入代码实战环节，女士们、先生们，集中注意力啦！先介绍一下今天的数据集<font color="red">(在Machine_Learning领域，数据集很重要，曾经小亮在听讲座的时候有牛人说，Deep_Learning时代是靠数据驱动的，数据就是血液，是宝贵的资源。所以，大家一定要重视数据！)</font>我们选用国外的Social_Network_Ads数据</strong>，该数据集包含了社交网络中用户的信息。这些信息涉及 <strong>用户ID,性别,年龄以及预估薪资。</strong>一家汽车公司刚刚推出了他们新型的豪华SUV，我们尝试预测哪些用户会购买这种全新SUV。并且在最后一列用来表示用户是否购买。<strong>我们将建立一种模型来预测用户是否购买这种SUV，该模型基于两个变量，分别是年龄和预计薪资。因此我们的特征矩阵将是这两列。我们尝试寻找用户年龄与预估薪资之间的某种相关性，以及他是否购买SUV的决定。数据结构如下图所示：</strong><br><img src="/NextLegend.github.io/2018/10/07/机器学习Day6/data.png" alt="机器学习Day6"></p><p><center><font color="black" size="4"><strong>三、实践之</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>好，数据结构介绍完了，我们开始写代码啦！<font color="red">(大家以后写代码一定要养成一个良好的习惯，在写代码之前，一定要分析清楚自己这个程序需要几个步骤step，也就是需要几个模块，每个步骤需要什么库和函数，每一部分之间有什么联系，这个思考很重要！！！</font>小亮本科时的C++老师曾说过：<font color="purple">三分写代码，七分调试代码，小亮觉得很有道理，但是小亮想再加一句,优化一下：三分设计代码、三分写代码、四分debug代码！！！</font>是不是很有道理？)我们今天的这个程序主要分为五个大步骤，如下所示：</strong><br>&#160; &#160; &#160; &#160;<strong><font color="blue">Step1:数据预处理。</font></strong><br>&#160; &#160; &#160; &#160;<strong>(1)导入相关的python库。我们主要用到numpy、pandas、matplotlib以及sklearn库。</strong><br>&#160; &#160; &#160; &#160;<strong>(2)导入数据集，就是我们上面所展示的数据集Social_Network_Ads</strong><br>&#160; &#160; &#160; &#160;<strong>(3)将数据集分成训练集和测试集。</strong><br>&#160; &#160; &#160; &#160;<strong>(4)特征缩放。</strong><br>&#160; &#160; &#160; &#160;<strong><font color="blue">Step2:建立逻辑回归模型。</font>这一步的python库将会是一个线性模型库，之所以被称为线性是因为逻辑回归是一个线性分类器，这意味着我们在二维空间中，我们两类用户（购买和不购买）将被一条直线分割。然后导入逻辑回归类。下一步我们将创建该类的对象，它将作为我们训练集的分类器。</strong><br>&#160; &#160; &#160; &#160;<strong>(1)将逻辑回归应用于训练集</strong><br>&#160; &#160; &#160; &#160;<strong><font color="blue">Step3:预测。</font></strong><br>&#160; &#160; &#160; &#160;<strong>(1)预测测试集结果。</strong><br>&#160; &#160; &#160; &#160;<strong><font color="blue">Step4:评估预测。</font>我们预测了测试集。 现在我们将评估逻辑回归模型是否正确的学习和理解。因此这个混淆矩阵将包含我们模型的正确和错误的预测。</strong><br>&#160; &#160; &#160; &#160;<strong>(1)生成混淆矩阵</strong><br>&#160; &#160; &#160; &#160;<strong><font color="blue">Step5:调用matplotlib库可视化。</font></strong></p><p><center><font color="black" size="4"><strong>四、结果之</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>下面就是我们通过对Social_Network_Ads数据集建立逻辑回归产生的结果：</strong><br><img src="/NextLegend.github.io/2018/10/07/机器学习Day6/result1.JPG" alt="机器学习Day6"><br><img src="/NextLegend.github.io/2018/10/07/机器学习Day6/result2.JPG" alt="机器学习Day6"></p><p><center><font color="black" size="4"><strong>五、代码之</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>下面就是我们本节内容建立逻辑回归所需要的全部代码：</strong></p><pre><code>#coding:utf-8#Step 1:导入库import numpy as npimport matplotlib.pyplot as pltimport pandas as pd#Step 2:导入数据集dataset = pd.read_csv(&apos;Social_Network_Ads.csv&apos;)X = dataset.iloc[:, [2, 3]].valuesY = dataset.iloc[:,4].values#Step 3:将数据集分成训练集和测试集from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)#Step 4:特征缩放from sklearn.preprocessing import StandardScalersc = StandardScaler()X_train = sc.fit_transform(X_train)X_test = sc.transform(X_test)#Step 5:逻辑回归模型,将逻辑回归应用于训练集from sklearn.linear_model import LogisticRegressionclassifier = LogisticRegression()classifier.fit(X_train, y_train)#Step 6: 预测,预测测试集结果y_pred = classifier.predict(X_test)#Step 7:评估预测我们预测了测试集。 生成混淆矩阵from sklearn.metrics import confusion_matrixcm = confusion_matrix(y_test, y_pred)#Step 8:可视化from matplotlib.colors import ListedColormapX_set,y_set=X_train,y_trainX1,X2=np. meshgrid(np. arange(start=X_set[:,0].min()-1, stop=X_set[:, 0].max()+1, step=0.01),                   np. arange(start=X_set[:,1].min()-1, stop=X_set[:,1].max()+1, step=0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),             alpha = 0.75, cmap = ListedColormap((&apos;red&apos;, &apos;green&apos;)))plt.xlim(X1.min(),X1.max())plt.ylim(X2.min(),X2.max())for i,j in enumerate(np. unique(y_set)):    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],                c = ListedColormap((&apos;red&apos;, &apos;green&apos;))(i), label=j)plt. title(&apos; LOGISTIC(Training set)&apos;)plt. xlabel(&apos; Age&apos;)plt. ylabel(&apos; Estimated Salary&apos;)plt. legend()plt. show()X_set,y_set=X_test,y_testX1,X2=np. meshgrid(np. arange(start=X_set[:,0].min()-1, stop=X_set[:, 0].max()+1, step=0.01),                   np. arange(start=X_set[:,1].min()-1, stop=X_set[:,1].max()+1, step=0.01))plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),             alpha = 0.75, cmap = ListedColormap((&apos;red&apos;, &apos;green&apos;)))plt.xlim(X1.min(),X1.max())plt.ylim(X2.min(),X2.max())for i,j in enumerate(np. unique(y_set)):    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],                c = ListedColormap((&apos;red&apos;, &apos;green&apos;))(i), label=j)plt. title(&apos; LOGISTIC(Test set)&apos;)plt. xlabel(&apos; Age&apos;)plt. ylabel(&apos; Estimated Salary&apos;)plt. legend()plt. show()</code></pre>]]></content>
    
    <summary type="html">
    
      今天是2018年10月7日，今天咱们继续“逻辑回归”系列之第三讲，“逻辑回归之代码实战”，这一讲将是检验大家前两将学习的最好方式，所以大家一定要重视哈！代码不会的就夯实代码功底，Come_on!继续跟随小亮的步伐，前进、前进、前进进！！！
    
    </summary>
    
      <category term="Machine_Learning Python" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/Machine-Learning-Python/"/>
    
    
      <category term="机器学习" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习Day5</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/10/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Day5/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/10/04/机器学习Day5/</id>
    <published>2018-10-04T08:54:34.000Z</published>
    <updated>2018-10-04T14:01:38.204Z</updated>
    
    <content type="html"><![CDATA[<p><center><font color="black" size="6"><strong>Day5逻辑回归之数学原理</strong></font></center><br>&#160; &#160; &#160; &#160;在开始枯燥的数学理论之前，小亮在带领大家回顾一下“Logistic_regression”哈，这次以八卦的方式来回味，哈哈。<strong>Logistic_regression可以用来回归，也可以用来分类，主要是二分类。大家应该了解点支持向量机SVM吧，(好吧，不了解，没关系哈，后面小亮会总结一下经典的机器学习算法) 其实它就是个二分类的算法，它可以将两个不同类别的样本给分开，<font color="red">思想是找到最能区分它们的那个分类超平面。但当你给一个新的样本给它，它能够给你的只有一个答案，你这个样本是正类还是负类。</font>例如你问SVM，某个女生是否喜欢你，它只会回答你喜欢或者不喜欢。这对我们来说，显得太粗鲁了，要不希望，要不绝望，这都不利于身心健康。那如果它可以告诉你，她很喜欢、有一点喜欢、不怎么喜欢或者一点都不喜欢，你想都不用想了等等，告诉你她有49%的几率喜欢你，总比直接说她不喜欢你，来得温柔。而且还提供了额外的信息，她来到你的身边你有多少希望，你得再努力多少倍，知己知彼百战百胜，哈哈。Logistic_regression就是显得这么温柔，它给我们提供的就是你的这个样本属于正类的可能性是多少。</strong><br><img src="/NextLegend.github.io/2018/10/04/机器学习Day5/Love.png" alt="机器学习Day5"></p><p><center><font color="black" size="4"><strong>一、数学原理</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>数学原理的解读总离不开实例，假设我们的样本是{x, y}，y是0或者1，表示正类或者负类，x是我们的m维的样本特征向量。那么这个样本x属于正类，也就是y=1的“概率”可以通过下面的逻辑函数来表示：</strong><br><img src="/NextLegend.github.io/2018/10/04/机器学习Day5/001.png" alt="机器学习Day5"><br><strong>这里θ是模型参数，也就是回归系数(这个参数很重要，大家先把它记下来。)，σ是sigmoid函数。实际上这个函数是由下面的对数几率（也就是x属于正类的可能性和负类的可能性的比值的对数）变换得到的：</strong><br><img src="/NextLegend.github.io/2018/10/04/机器学习Day5/002.png" alt="机器学习Day5"><br>&#160; &#160; &#160; &#160;<strong>换句话说，y也就是我们关系的变量，</strong>例如她喜不喜欢你，与多个自变量（因素）有关，例如你人品怎样、车子是两个轮的还是四个轮的、长得胜过潘安还是和犀利哥有得一拼、有千尺豪宅还是三寸茅庐等等，我们把这些因素表示为x1, x2,…, xm。那这个女的怎样考量这些因素呢？<strong>最快的方式就是把这些因素的得分都加起来，最后得到的和越大，就表示越喜欢。但每个人心里其实都有一杆称，每个人考虑的因素不同，萝卜青菜，各有所爱嘛。</strong>例如这个女生更看中你的人品，人品的权值是0.6，不看重你有没有钱，没钱了一起努力奋斗，那么有没有钱的权值是0.001等等。我们将这些对应x1, x2,…, xm的权值叫做回归系数，表达为θ1,θ2,…,θm。他们的加权和就是你的总得分了。请选择你的心仪男生，非诚勿扰！哈哈。</p><p><center><font color="black" size="4"><strong>二、再次解读</strong></font></center><br>&#160; &#160; &#160; &#160;上面的logistic回归是一个线性分类模型，<strong>它与线性回归的不同点在于：为了将线性回归输出的很大范围的数，例如从负无穷到正无穷，压缩到0和1之间，这样的输出值表达为“可能性”才能说服广大民众。当然了，把大值压缩到这个范围还有个很好的好处，就是可以消除特别冒尖的变量的影响（不知道理解的是否正确）。</strong>而实现这个伟大的功能其实就只需要平凡一举，也就是在输出加一个logistic函数。<strong>对于二分类来说，可以简单的认为：如果样本x属于正类的概率大于0.5，那么就判定它是正类，否则就是负类。</strong>实际上，SVM的类概率就是样本到边界的距离，这个活实际上就让logistic regression给做了。</p><p><center><font color="black" size="4"><strong>三、神经网络模型</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>Logistic_regression的神经网络模型如下图所示：<font color="red">(大家不懂这些神经网络基本概念的可以参考小亮之前的博客《神经网络的基本概念》<a href="https://blog.csdn.net/jinyuan7708/article/details/82466653" target="_blank" rel="noopener">https://blog.csdn.net/jinyuan7708/article/details/82466653</a>)</font></strong><br><img src="/NextLegend.github.io/2018/10/04/机器学习Day5/003.png" alt="机器学习Day5"></p><p><center><font color="black" size="4"><strong>四、小亮总结</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>今天我们大概讲了下逻辑回归的数学原理，其实简单点说Logistic_Regression就是一个被logistic方程归一化后的线性回归。最终是用作分类器：从样本集中学习拟合参数，将目标值拟合到[0,1]之间，然后对目标值进行离散化，实现分类，仅此而已。今天的内容掌握了吗？同学们还需要多看看相关的资料哈，下一节小亮带你手撸代码，看看到底什么是Logistic_Regression！！！</strong></p>]]></content>
    
    <summary type="html">
    
      今天是2018年10月4日，今天咱们继续“逻辑回归”系列之第二讲，“逻辑回归之数学原理”，这一讲很重要哈，可能有些枯燥，毕竟涉及到公式了，但是小亮尽自己最大的努力讲的通俗易懂，以最直白的方式让大家掌握“逻辑回归”背后的秘密。Come_on!继续跟随小亮的步伐，前进、前进！！！
    
    </summary>
    
      <category term="Machine_Learning Python" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/Machine-Learning-Python/"/>
    
    
      <category term="机器学习" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习Day4</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/10/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Day4/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/10/04/机器学习Day4/</id>
    <published>2018-10-04T03:10:39.000Z</published>
    <updated>2018-10-04T08:51:07.543Z</updated>
    
    <content type="html"><![CDATA[<p><center><font color="black" size="6"><strong>Day4逻辑回归之基本概念</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>先给大家看一下逻辑回归的基本概念哈，<font color="red">大家先对这个有一个直观的认识就好，看不懂或者不明白没关系哈，后面小亮会一点一点给大家答疑解惑，拨开云雾，柳暗花明哈</font>。逻辑回归如下面这幅图片所示：</strong><br><img src="/NextLegend.github.io/2018/10/04/机器学习Day4/Day4.jpg" alt="机器学习Day4"></p><p><center><font color="black" size="4"><strong>一、直观概念</strong></font></center><br>&#160; &#160; &#160; &#160;逻辑回归，英文为logistic，又称logistic回归分析，是一种广义的线性回归分析模型，常用于数据挖掘，疾病自动诊断，经济预测等领域。<strong>例如，以胃癌病情分析为例，选择两组人群，一组是胃癌组，一组是非胃癌组，两组人群必定具有不同的体征与生活方式等。因此因变量就为是否胃癌，值为“是”或“否”，自变量就可以包括很多了，如年龄、性别、饮食习惯、幽门螺杆菌感染等。自变量既可以是连续的，也可以是分类的。然后通过logistic回归分析，可以得到自变量的权重，从而可以大致了解到底哪些因素是胃癌的危险因素。同时根据该权值可以根据危险因素预测一个人患癌症的可能性。</strong></p><p><center><font color="black" size="4"><strong>二、数学概念</strong></font></center><br>&#160; &#160; &#160; &#160;logistic回归是一种广义线性回归（generalized linear model），因此与多重线性回归分析有很多相同之处。<strong>它们的数学模型形式基本上相同，都具有 w‘x+b，其中w和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将w‘x+b作为因变量，即y =w‘x+b，<font color="red">而logistic回归则通过函数L将w‘x+b对应一个隐状态p，p =L(w‘x+b),然后根据p 与1-p的大小决定因变量的值。</font>如果L是logistic函数，就是logistic回归，如果L是多项式函数就是多项式回归</strong></p><p><center><font color="black" size="4"><strong>三、小亮总结</strong></font></center><br>&#160; &#160; &#160; &#160;逻辑回归其实就是这样的一个过程：<strong><font color="red">面对一个回归或者分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证我们这个求解的模型的好坏。</font></strong>Logistic回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别）回归模型中，y是一个定性变量，比如y=0或1，logistic方法主要应用于研究某些事件发生的概率。</p><p><center><font color="black" size="4"><strong>四、常用的logistic函数</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>sigmoid函数为常用的logistic函数，数学公式如下图所示：</strong><br><img src="/NextLegend.github.io/2018/10/04/机器学习Day4/sigmoid.png" alt="机器学习Day4"><br>&#160; &#160; &#160; &#160;<strong>小亮再用代码给大家画一下这个函数：</strong><br><img src="/NextLegend.github.io/2018/10/04/机器学习Day4/sigmoid画图.JPG" alt="机器学习Day4"></p><pre><code>#coding:utf-8import matplotlib.pyplot as pltimport numpy as npdef Sigmoid(x):    return 1.0 / (1.0 + np.exp(-x))x = np.arange(-20, 20, 0.1)h = Sigmoid(x)  # Sigmoid函数plt.plot(x, h,color=&apos;red&apos;)plt.axvline(0.0, color=&apos;k&apos;)  # 坐标轴上加一条竖直的线（0位置）plt.axhspan(0.0, 1.0, facecolor=&apos;1.0&apos;, alpha=1.0, ls=&apos;dotted&apos;)plt.axhline(y=0.5, ls=&apos;dotted&apos;, color=&apos;k&apos;)plt.yticks([0.0, 0.5, 1.0])  # y轴标度plt.ylim(-0.1, 1.1)  # y轴范围plt.show()</code></pre>]]></content>
    
    <summary type="html">
    
      今天是2018年10月4日，小亮前两天在忙其他的事情，没有来得及学习及更新机器学习Every_Day，小亮在这里向大家道歉哈！今天将过去几天未更新的都补回来，将功补过。未来的三篇博客都将围绕“逻辑回归”展开，分别为“逻辑回归之基本概念”、“逻辑回归之数学原理”、“逻辑回归之代码实战”。今天的学习内容为“逻辑回归之基本概念”，Come on!跟随小亮的步伐，前进前进！！！
    
    </summary>
    
      <category term="Machine_Learning Python" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/Machine-Learning-Python/"/>
    
    
      <category term="机器学习" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>2018知识图谱发展报告之小亮见解</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/10/02/2018%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%8F%91%E5%B1%95%E6%8A%A5%E5%91%8A%E4%B9%8B%E5%B0%8F%E4%BA%AE%E8%A7%81%E8%A7%A3/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/10/02/2018知识图谱发展报告之小亮见解/</id>
    <published>2018-10-02T02:10:19.000Z</published>
    <updated>2018-10-02T16:07:43.332Z</updated>
    
    <content type="html"><![CDATA[<p><center><font color="black" size="6"><strong>2018知识图谱发展报告之小亮见解</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>先给大家把此次2018知识图谱的发展报告PDF版送上，封面如下面这幅图片所示：</strong><br><strong>百度云链接为：链接：<a href="https://pan.baidu.com/s/1qI7pYp03NOTL4V9cQ9GV2g" target="_blank" rel="noopener">https://pan.baidu.com/s/1qI7pYp03NOTL4V9cQ9GV2g</a> 密码：kewf</strong><br><img src="/NextLegend.github.io/2018/10/02/2018知识图谱发展报告之小亮见解/知识图谱.JPG" alt="2018知识图谱发展报告之小亮见解"><br>&#160; &#160; &#160; &#160;<strong>下面是这份知识图谱的目录和编写人员，可以看到大多数是领域内知名专家和教授，比如小亮目前所在的事件检测方向的赵军老师和他的学生陈玉博老师等等。</strong><br><img src="/NextLegend.github.io/2018/10/02/2018知识图谱发展报告之小亮见解/目录.JPG" alt="2018知识图谱发展报告之小亮见解"><br><img src="/NextLegend.github.io/2018/10/02/2018知识图谱发展报告之小亮见解/作者.JPG" alt="2018知识图谱发展报告之小亮见解"><br>&#160; &#160; &#160; &#160;<strong>小亮对这份知识图谱领域的发展报告大致看了一遍，其中对于第五章《事件知识学习》看的比较详细，因为目前小亮对这个领域内的任务比较明确，下面就讲讲小亮之所见吧。</strong><br>&#160; &#160; &#160; &#160;<strong>事件识别和抽取研究如何从描述事件信息的文本中识别并抽取出事件信息并以结构化的形式呈现出来，包括其发生的时间、地点、参与角色以及与之相关的动作或者状态的改变，核心的概念如下图所示：</strong><br><img src="/NextLegend.github.io/2018/10/02/2018知识图谱发展报告之小亮见解/事件检测.JPG" alt="2018知识图谱发展报告之小亮见解"><br>&#160; &#160; &#160; &#160;<strong>事件检测与追踪</strong>旨在将文本新闻流按照其报道的事件进行组织，为传统媒体多种来源的新闻监控提供核心技术，以便让用户了解新闻及其发展。具体而言，<strong>事件发现与跟踪包括三个主要任务：分割，发现和跟踪，</strong>将新闻文本分解为事件，发现新的（不可预见的）事件，并跟踪以前报道事件的发展。事件发现任务又可细分为历史事件发现和在线事件发现两种形式，前者目标是从按时间排序的新闻文档中发现以前没有识别的事件，后者则是从实时新闻流中实时发现新的事件。</p><p><center><font color="black" size="4"><strong>数据集</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>现在不论是自然语言处理领域，还是计算机视觉领域，都热衷于神经网络Embedding方法，自然少不了对于数据集的选取。那么，Event Detection领域的标准数据集是ACE2005语料。详细的语料介绍如下所示：</strong><br><img src="/NextLegend.github.io/2018/10/02/2018知识图谱发展报告之小亮见解/ACE.JPG" alt="2018知识图谱发展报告之小亮见解"><br><img src="/NextLegend.github.io/2018/10/02/2018知识图谱发展报告之小亮见解/中文事件语料.JPG" alt="2018知识图谱发展报告之小亮见解"><br>&#160; &#160; &#160; &#160;ACE语料是需要花钱买的，现在网上是找不到免费的语料的。出于对中文事件语料Chinese Event Corpus, CEC的好奇，小亮找到了这个语料(大家如果做研究用，可以免费下载哈！)：</p><p><center><strong>链接如下：<a href="https://github.com/shijiebei2009/CEC-Corpus" target="_blank" rel="noopener">https://github.com/shijiebei2009/CEC-Corpus</a></strong></center><br>&#160; &#160; &#160; &#160;中文突发事件语料库是由上海大学（语义智能实验室）所构建。根据国务院颁布的《国家突发公共事件总体应急预案》的分类体系，从互联网上收集了5类（地震、火灾、交通事故、恐怖袭击和食物中毒）突发事件的新闻报道作为生语料，然后再对生语料进行文本预处理、文本分析、事件标注以及一致性检查等处理，最后将标注结果保存到语料库中，CEC合计332篇。</p><p><center><font color="black" size="4"><strong>技术路线</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>事件抽取当前技术路线为基于模识匹配的事件抽取和基于机器学习的事件抽取。其中，基于模识匹配的事件抽取是基于传统的方法来做，需要定义模式和规则，比较严谨，在特定领域中性能较好，表示简洁，但对于语言、领域和文档形式等均有不同程度的依赖，覆盖度和可移植性较差。基于机器学习的方法建立在统计模型基础上，一般将事件抽取建模成多分类问题，因此研究的重点在于特征和分类器的选择。</strong><br><img src="/NextLegend.github.io/2018/10/02/2018知识图谱发展报告之小亮见解/局部信息与全局信息.JPG" alt="2018知识图谱发展报告之小亮见解"><br>这块将局部信息与全局信息的融合思想，小亮觉得可以重点研究一下可行性问题和性能，这个idea还是不错的。</p><p><center><font color="black" size="4"><strong>机遇与挑战</strong></font></center><br>&#160; &#160; &#160; &#160;目前国内外事件抽取相关的研究大部分都是面向英文文本的英文事件抽取，<strong>面向中文文本的中文事件抽取工作才刚刚起步，主要面临技术和数据两方面的挑战。技术层面，</strong>中文的词句是意合的，词语间没有显式分隔符，而且中文实词在时态和形态上也没有明显变化， 因此面向中文的事件抽取研究在基础自然语言处理层面具有天然的劣势。 <strong>数据层面，</strong> 由于起步较晚，缺乏统一的、公认的语料资源和相关评测，极大制约了中文事件抽取的研究。尽管如此，近些年中文事件抽取在公开评测、领域扩展和跨预料迁移方面也取得一定进展。<strong>所以，小亮觉得，中文场景下的事件抽取拥有更大的发展潜力与空间，以后小亮还会持续关注这个了领域的。</strong></p><p><center><font color="black" size="4"><strong>小亮说</strong></font></center><br>&#160; &#160; &#160; &#160;最后，小亮觉得知识图谱也是这两年一下就火起来了，还处于一个萌芽的阶段，对这个领域的技术小亮心里还抱有迟疑的感觉，不过未来知识图谱的发展一定是空前的，可能三五年后吧，知识图谱将会改变传统的搜索引擎的模式，领域内知识图谱会更多，更成熟，这也是一个非常好的机会。明天或者后天小亮会实践一下知识图谱这个领域的技术，切身感受一下它的威力。以上就是小亮对于2018年知识图谱发展报告的一个小小的感受，欢迎大家交流哈！</p>]]></content>
    
    <summary type="html">
    
      今天是2018年10月2日，在庆祝我们伟大的祖国六十九周年生日的国庆七天假期里，小亮和众多默默奉献的人民子弟兵、工人一样，坚守在自己的岗位上，试图尽自己的一份微弱的光芒，照亮神州大地。话不多说，直接上干货，今天小亮细看了一下2018知识图谱发展报告，对于刚步入这个领域以及刚刚开始从事科研的小新们是一盏前行的明灯哈，下面且听小亮Share.
    
    </summary>
    
      <category term="自然语言处理 知识图谱 Knowledge_Graph" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1-Knowledge-Graph/"/>
    
    
      <category term="自然语言处理" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>机器学习Day3</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Day3/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/09/30/机器学习Day3/</id>
    <published>2018-09-29T16:17:40.000Z</published>
    <updated>2018-09-29T16:35:20.137Z</updated>
    
    <content type="html"><![CDATA[<center><font color="black" size="6"><strong>Day3多元线性回归</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>先给大家看一下这一部分的流程哈，主要分为3个Step，如下面这幅图片所示：</strong><br><img src="/NextLegend.github.io/2018/09/30/机器学习Day3/Day3.png" alt="机器学习Day3"><br>    #coding:utf-8<br>    # 导入库<br>    import pandas as pd<br>    import numpy as np<br><br>    #Step1: 导入数据集<br>    dataset = pd.read_csv(‘50_Startups.csv’)<br>    X = dataset.iloc[ : , :-1].values<br>    Y = dataset.iloc[ : ,  4 ].values<br><br>    # Step2: 将类别数据数字化<br>    from sklearn.preprocessing import LabelEncoder, OneHotEncoder<br>    labelencoder = LabelEncoder()<br>    X[: , 3] = labelencoder.fit_transform(X[ : , 3])<br>    onehotencoder = OneHotEncoder(categorical_features = [3])<br>    X = onehotencoder.fit_transform(X).toarray()<br><br>    #Step3:躲避虚拟变量陷阱<br>    X = X[: , 1:]<br><br>    # 拆分数据集为训练集和测试集<br>    from sklearn.model_selection import train_test_split<br>    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)<br><br>    # 第2步： 在训练集上训练多元线性回归模型<br>    from sklearn.linear_model import LinearRegression<br>    regressor = LinearRegression()<br>    regressor.fit(X_train, Y_train)<br><br>    # Step4: 在测试集上预测结果<br>    y_pred = regressor.predict(X_test)<br>    print(y_pred)<br><br><img src="/NextLegend.github.io/2018/09/30/机器学习Day3/result.JPG" alt="机器学习Day3"><br><center><strong>测试集上预测结果</strong></center>]]></content>
    
    <summary type="html">
    
      今天是2018年9月29日，小亮开始了机器学习Day3，今天学习多元线性内容为多元线回归部分，多元线性回归尝试通过用一个线性方程来适配观测数据，这个线性方程是在两个以上（包括两个）的特征和响应之间构建的一个关系。多元线性回归的实现步骤和简单线性回归很相似，在评价部分有所不同。Python库会用到Numpy、Pandas、Sklearn等。
    
    </summary>
    
      <category term="Machine_Learning Python" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/Machine-Learning-Python/"/>
    
    
      <category term="机器学习" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习Day2</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/09/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Day2/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/09/28/机器学习Day2/</id>
    <published>2018-09-28T08:59:44.000Z</published>
    <updated>2018-09-28T09:19:25.218Z</updated>
    
    <content type="html"><![CDATA[<center><font color="black" size="6"><strong>Day2简单线性回归</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>先给大家看一下这一部分的流程哈，主要分为4个Step，如下面这幅图片所示：</strong><br><img src="/NextLegend.github.io/2018/09/28/机器学习Day2/Day2.jpg" alt="机器学习Day2"><br><br>        #coding:utf-8<br>        import pandas as pd<br>        import numpy as np<br>        import matplotlib.pyplot as plt<br>        #Step1:数据预处理<br>        dataset = pd.read_csv(‘studentscores.csv’)<br>        X = dataset.iloc[ : ,   : 1 ].values<br>        Y = dataset.iloc[ : , 1 ].values<br>        from sklearn.model_selection import train_test_split<br>        X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = 1/4, random_state = 0)<br>        #Step2:训练集使用简单线性回归模型来训练<br>        from sklearn.linear_model import LinearRegression<br>        regressor = LinearRegression()<br>        regressor = regressor.fit(X_train, Y_train)<br>        #Step3：预测结果<br>        Y_pred = regressor.predict(X_test)<br>        #Step4:可视化<br>        #训练集结果可视化<br>        plt.scatter(X_train , Y_train, color = ‘red’)<br>        plt.plot(X_train , regressor.predict(X_train), color =’blue’)<br>        plt.show()<br>        #测试集结果可视化<br>        plt.scatter(X_test , Y_test, color = ‘red’)<br>        plt.plot(X_test , regressor.predict(X_test), color =’blue’)<br>        plt.show()<br><br>&#160; &#160; &#160; &#160;<strong>将上面的代码输入到编辑器中，执行，就会得到下面的结果，因为我们调用Matplotlib画图函数，所以我们可以得到可视化之后的结果，如下图所示：。</strong><br><img src="/NextLegend.github.io/2018/09/28/机器学习Day2/figure1.JPG" alt="机器学习Day2"><br><center><strong>训练集结果可视化</strong></center><br><img src="/NextLegend.github.io/2018/09/28/机器学习Day2/figure2.JPG" alt="机器学习Day2"><br><center><strong>测试集结果可视化</strong></center>]]></content>
    
    <summary type="html">
    
      今天是2018年9月28日，小亮开始了机器学习Day2，今天学习的内容为简单线性回归部分，主要学习根据已有的数据，如何拟合出一条直线，也就是构造出一个函数，能够表示已有数据的分布。Python库会用到Numpy、Pandas、Matplotlib等。
    
    </summary>
    
      <category term="Machine_Learning Python" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/Machine-Learning-Python/"/>
    
    
      <category term="机器学习" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习Day1</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/09/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Day1/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/09/27/机器学习Day1/</id>
    <published>2018-09-27T13:13:46.000Z</published>
    <updated>2018-09-27T13:34:40.054Z</updated>
    
    <content type="html"><![CDATA[<p><center><font color="black" size="6"><strong>Day1数据预处理</strong></font></center><br>&#160; &#160; &#160; &#160;<strong>先给大家看一下这一部分的流程哈，主要分为6个Step，如下面这幅图片所示：</strong><br><img src="/NextLegend.github.io/2018/09/27/机器学习Day1/Day1.jpg" alt="机器学习Day1"></p><pre><code>#coding:utf-8#Day 1: Data Prepocessing#Step 1: Importing the librariesimport numpy as npimport pandas as pd#Step 2: Importing datasetdataset = pd.read_csv(&apos;Data.csv&apos;)X = dataset.iloc[ : , :-1].valuesY = dataset.iloc[ : , 3].valuesprint(&quot;Step 2: Importing dataset&quot;)print(&quot;X&quot;)print(X)print(&quot;Y&quot;)print(Y)#Step 3: Handling the missing datafrom sklearn.preprocessing import Imputerimputer = Imputer(missing_values = &quot;NaN&quot;, strategy = &quot;mean&quot;, axis = 0)imputer = imputer.fit(X[ : , 1:3])X[ : , 1:3] = imputer.transform(X[ : , 1:3])print(&quot;---------------------&quot;)print(&quot;Step 3: Handling the missing data&quot;)print(&quot;step2&quot;)print(&quot;X&quot;)print(X)#Step 4: Encoding categorical datafrom sklearn.preprocessing import LabelEncoder, OneHotEncoderlabelencoder_X = LabelEncoder()X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])#Creating a dummy variableonehotencoder = OneHotEncoder(categorical_features = [0])X = onehotencoder.fit_transform(X).toarray()labelencoder_Y = LabelEncoder()Y =  labelencoder_Y.fit_transform(Y)print(&quot;---------------------&quot;)print(&quot;Step 4: Encoding categorical data&quot;)print(&quot;X&quot;)print(X)print(&quot;Y&quot;)print(Y)#Step 5: Splitting the datasets into training sets and Test setsfrom sklearn.model_selection import train_test_splitX_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)print(&quot;---------------------&quot;)print(&quot;Step 5: Splitting the datasets into training sets and Test sets&quot;)print(&quot;X_train&quot;)print(X_train)print(&quot;X_test&quot;)print(X_test)print(&quot;Y_train&quot;)print(Y_train)print(&quot;Y_test&quot;)print(Y_test)#Step 6: Feature Scalingfrom sklearn.preprocessing import StandardScalersc_X = StandardScaler()X_train = sc_X.fit_transform(X_train)X_test = sc_X.transform(X_test)print(&quot;---------------------&quot;)print(&quot;Step 6: Feature Scaling&quot;)print(&quot;X_train&quot;)print(X_train)print(&quot;X_test&quot;)print(X_test)</code></pre><p>&#160; &#160; &#160; &#160;<strong>将上面的代码输入到编辑器中，执行，就会得到下面的结果，大家先看看结果是不是和小亮一样哈，后面我们再解说一下数据处理的详细过程。</strong></p><pre><code>G:\Code\Python_Learn\Study_Tensorflow_2018\venv\Scripts\python.exe &quot;G:/Code/Python_Learn/Study_Tensorflow_2018/venv/Machine Learning 100 days/day1/day1.py&quot;Step 2: Importing datasetX[[&apos;France&apos; 44.0 72000.0] [&apos;Spain&apos; 27.0 48000.0] [&apos;Germany&apos; 30.0 54000.0] [&apos;Spain&apos; 38.0 61000.0] [&apos;Germany&apos; 40.0 nan] [&apos;France&apos; 35.0 58000.0] [&apos;Spain&apos; nan 52000.0] [&apos;France&apos; 48.0 79000.0] [&apos;Germany&apos; 50.0 83000.0] [&apos;France&apos; 37.0 67000.0]]Y[&apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;No&apos; &apos;Yes&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos; &apos;No&apos; &apos;Yes&apos;]---------------------Step 3: Handling the missing datastep2X[[&apos;France&apos; 44.0 72000.0] [&apos;Spain&apos; 27.0 48000.0] [&apos;Germany&apos; 30.0 54000.0] [&apos;Spain&apos; 38.0 61000.0] [&apos;Germany&apos; 40.0 63777.77777777778] [&apos;France&apos; 35.0 58000.0] [&apos;Spain&apos; 38.77777777777778 52000.0] [&apos;France&apos; 48.0 79000.0] [&apos;Germany&apos; 50.0 83000.0] [&apos;France&apos; 37.0 67000.0]]---------------------Step 4: Encoding categorical dataX[[1.00000000e+00 0.00000000e+00 0.00000000e+00 4.40000000e+01  7.20000000e+04] [0.00000000e+00 0.00000000e+00 1.00000000e+00 2.70000000e+01  4.80000000e+04] [0.00000000e+00 1.00000000e+00 0.00000000e+00 3.00000000e+01  5.40000000e+04] [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.80000000e+01  6.10000000e+04] [0.00000000e+00 1.00000000e+00 0.00000000e+00 4.00000000e+01  6.37777778e+04] [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.50000000e+01  5.80000000e+04] [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.87777778e+01  5.20000000e+04] [1.00000000e+00 0.00000000e+00 0.00000000e+00 4.80000000e+01  7.90000000e+04] [0.00000000e+00 1.00000000e+00 0.00000000e+00 5.00000000e+01  8.30000000e+04] [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.70000000e+01  6.70000000e+04]]Y[0 1 0 0 1 1 0 1 0 1]---------------------Step 5: Splitting the datasets into training sets and Test setsX_train[[0.00000000e+00 1.00000000e+00 0.00000000e+00 4.00000000e+01  6.37777778e+04] [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.70000000e+01  6.70000000e+04] [0.00000000e+00 0.00000000e+00 1.00000000e+00 2.70000000e+01  4.80000000e+04] [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.87777778e+01  5.20000000e+04] [1.00000000e+00 0.00000000e+00 0.00000000e+00 4.80000000e+01  7.90000000e+04] [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.80000000e+01  6.10000000e+04] [1.00000000e+00 0.00000000e+00 0.00000000e+00 4.40000000e+01  7.20000000e+04] [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.50000000e+01  5.80000000e+04]]X_test[[0.0e+00 1.0e+00 0.0e+00 3.0e+01 5.4e+04] [0.0e+00 1.0e+00 0.0e+00 5.0e+01 8.3e+04]]Y_train[1 1 1 0 1 0 0 1]Y_test[0 0]---------------------Step 6: Feature ScalingX_train[[-1.          2.64575131 -0.77459667  0.26306757  0.12381479] [ 1.         -0.37796447 -0.77459667 -0.25350148  0.46175632] [-1.         -0.37796447  1.29099445 -1.97539832 -1.53093341] [-1.         -0.37796447  1.29099445  0.05261351 -1.11141978] [ 1.         -0.37796447 -0.77459667  1.64058505  1.7202972 ] [-1.         -0.37796447  1.29099445 -0.0813118  -0.16751412] [ 1.         -0.37796447 -0.77459667  0.95182631  0.98614835] [ 1.         -0.37796447 -0.77459667 -0.59788085 -0.48214934]]X_test[[-1.          2.64575131 -0.77459667 -1.45882927 -0.90166297] [-1.          2.64575131 -0.77459667  1.98496442  2.13981082]]Process finished with exit code 0</code></pre><p>&#160; &#160; &#160; &#160;<strong>上面就是今天的机器学习Day1的内容，大家重点了解一下Numpy、Pandas、Sklearn这三个库的使用和数据处理部分。</strong></p>]]></content>
    
    <summary type="html">
    
      今天是2018年9月27日，小亮开始了机器学习Day1，今天学习的内容为数据预处理部分，主要学习Numpy、Pandas、Sklearn库的使用和数据处理。
    
    </summary>
    
      <category term="Machine_Learning Python" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/Machine-Learning-Python/"/>
    
    
      <category term="机器学习" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习100天</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/09/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0100%E5%A4%A9/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/09/27/机器学习100天/</id>
    <published>2018-09-27T12:49:16.000Z</published>
    <updated>2018-09-27T13:08:53.757Z</updated>
    
    <content type="html"><![CDATA[<p><center><font color="black" size="8"><strong>机器学习100天</strong></font></center><br>&#160; &#160; &#160; &#160;小亮接触深度学习也将近一年了，通过走了这么多的路，读论文也好，看视频也好，看书也好，<strong>发现还是得通过边敲代码边思考边理解公式这样的方式比较踏实</strong>，不然就是天马行空，吹吹牛罢了！你会什么？我会深度学习耶！那你给我写两行代码解决一下这个问题？感知机是什么？交叉熵损失函数是什么？反向转播算法来推导一下？Pandas、Numpy、Scikit-learn这些库用过吗？<strong>当面试官或者HR问起这些的时候，我希望小亮或者你们能够胸有成竹的说，这些我都会哈，来我给你手工推导一下交叉熵损失函数是如何影响网络的学习速率的，反向传播算法是根据微积分的链式求导罚则调节参数权重w和偏置b的，这个项目我做过，那个我也知道。。。。。。其实，这个时候这种状态才是小亮应有的状态，希望在2020年毕业的时候，再回到此处时，可以做到无悔于时间、无悔于现在所做的一切与努力。每天进步一点点，来跟着小亮Machine Learning吧！</strong><br><img src="/NextLegend.github.io/2018/09/27/机器学习100天/001.JPG" alt="机器学习100天"><br><img src="/NextLegend.github.io/2018/09/27/机器学习100天/002.JPG" alt="机器学习100天"><br><img src="/NextLegend.github.io/2018/09/27/机器学习100天/003.JPG" alt="机器学习100天"><br><img src="/NextLegend.github.io/2018/09/27/机器学习100天/004.JPG" alt="机器学习100天"><br><img src="/NextLegend.github.io/2018/09/27/机器学习100天/005.JPG" alt="机器学习100天"><br>&#160; &#160; &#160; &#160;<strong>上面的是机器学习100天的每天的内容，从今天开始，小亮将会身体力行的实践，边思考边前行边总结边记录每一天的成长与收获！欢迎各位小伙伴与小亮一起前行，不断地打怪升级！</strong></p>]]></content>
    
    <summary type="html">
    
      今天是2018年9月27日，距离2019年春节还有96天的时间，小100天。小亮开始了Machine_Learning_100_days的学习，今天先介绍一下自己的学习计划与课程相关内容，欢迎大家与小亮实时交流哈！
    
    </summary>
    
      <category term="Machine_Learning Python" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/Machine-Learning-Python/"/>
    
    
      <category term="机器学习" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>NLP事件检测基本概念</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/09/25/NLP%E4%BA%8B%E4%BB%B6%E6%A3%80%E6%B5%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/09/25/NLP事件检测基本概念/</id>
    <published>2018-09-25T13:39:42.000Z</published>
    <updated>2018-09-25T14:17:10.959Z</updated>
    
    <content type="html"><![CDATA[<p><center><font color="black" size="16"><strong>自然语言处理之事件检测</strong></font></center><br><strong>一、什么是NLP</strong><br>&#160; &#160; &#160; &#160;nlp是自然语言处理，是电脑理解并表达出人们平常的所说的语言<br><strong>二、nlp的事件抽取是什么？</strong><br>&#160; &#160; &#160; &#160;事件抽取是从非结构信息中抽取出用户感兴趣的信息，并以结构化数据传递给用户。<br><strong>三、事件抽取所处的位置？</strong><br>&#160; &#160; &#160; &#160;事件抽取是信息抽取的一部分。事件抽取的又分为元事件抽取和主题事件抽取。元事件抽取是动作状态级的，动作产生或状态发生变化，一般由动词驱动。主题事件抽取是事件级的，一类核心事件或活动以及与他们相关的事件和活动。<br><strong>四、事件抽取的研究方法有哪些？</strong><br>&#160; &#160; &#160; &#160;事件抽取的研究方法有模式匹配和机器学习两种。模式匹配只针对特定领域，移植性差。机器学习应用广泛，移植性好。<br><strong>五、模式匹配方法如何进行事件抽取？</strong><br>&#160; &#160; &#160; &#160;模式匹配方法是在一定模式的指导下进行事件的识别和抽取。<br>模式：指的是抽取模式。通过领域知识和语言知识对目标信息的上下文环境进行约束。而这约束条件就是抽取模式。另外模式是手工建立的，耗时又费力，所以现在用的都是机器学习方法的事件抽取。<br><strong>六、机器学习方法如何进行事件抽取？</strong><br>&#160; &#160; &#160; &#160;对元事件抽取两大主要任务：对事件识别与分类和对事件元素进行识别和分类。事件元素识别和分类是事件识别和分类的基础。有关论文显示：机器学习算法混合使用将优于单一算法。事件的探测分两种实现方式：基于触发词的探测方式和基于事件的事例的探测方式。<br>&#160; &#160; &#160; &#160;基于触发词的探测方式：<br>&#160; &#160; &#160; &#160;基于触发词的探测方式的有正反例不平衡和数据稀疏的缺点。因为只有少量触发词作为输入数据进行训练，大量未参与进来的。作为反例数据参与到模型中，造成正反例不平衡，触发词数据稀疏。解决触发词探测缺点的方法：通过同义词扩展和二分类结合的方法进行解决，即将触发词放入词典中进行同义词扩展。<br>&#160; &#160; &#160; &#160;基于事件实例的探测方式：<br>&#160; &#160; &#160; &#160;基于事件实例的探测方式是将句子而不是词语作为识别实例。进而通过聚类方法转化为句子聚类问题，通过聚类得到事件句。避开了基于触发词探测的缺点。<br><strong>七、基于机器学习方法抽取方式的特点？</strong><br>&#160; &#160; &#160; &#160;(1) 机器学习方法的优点是自动获取模式。<br>&#160; &#160; &#160; &#160;(2) 机器学习方法不基于语料的格式和内容，但需要大量标准预料（解决方法:无监督和半监督的方法）</p>]]></content>
    
    <summary type="html">
    
      今天和明天小亮要整理NLP事件抽取方向的各个顶会Paper，梳理一下知识点找一些idea 所以今天先给大家普及一下自然语言处理领域中事件检测方向的一些基本概念等等！
    
    </summary>
    
      <category term="NLP Event Detection" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/NLP-Event-Detection/"/>
    
    
      <category term="NLP" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>这里的人与这里的故事</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/09/09/%E8%BF%99%E9%87%8C%E7%9A%84%E4%BA%BA%E4%B8%8E%E8%BF%99%E9%87%8C%E7%9A%84%E6%95%85%E4%BA%8B/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/09/09/这里的人与这里的故事/</id>
    <published>2018-09-09T14:11:16.000Z</published>
    <updated>2018-09-09T16:07:05.728Z</updated>
    
    <content type="html"><![CDATA[<p>&#160; &#160; &#160; &#160;在记录自己今天的感受前，先来介绍一下我大学仅有的几个好朋友之一———&gt;<strong>老郭</strong>（也就是今天的主人公李同学）！<strong>我和老郭的认识源于电子设计比赛，认知于电子设计比赛，结交于电子设计比赛，虽然我们不是一个班的，但是我们在很多问题上交流的很多，从他身上也学到了很多为人处世的道理——&gt;谦逊、踏实、担当，还有感恩！</strong><br>&#160; &#160; &#160; &#160;早上十点钟我们本来约定在大学城地铁站见面，但是很巧的是，在营口道转3号线的时候，小亮竟然与老郭完美的偶遇于一趟车的相邻车厢。（这就是缘分！哈哈！）上车后第一眼就看到了他，远远地看去，瘦了一圈，可能是来自于工作的压力人就会瘦吧！身穿一件深灰色的衬衣与牛仔裤，戴着耳机，（程序员可能都是这样的装备吧！）也在寻觅着我。老郭还是老郭，还是那样的幽默风趣，还是那样的思考着前行着前行着而又思考着，其实小亮心里一直挺为他惋惜的，但是一直又鼓励着他，让他自信起来，相信自己，不要因为过去的事带着自卑的情绪而影响现在的自己，因为你值得更好地未来，没错，你值得！！！我们俩就像失散多年的老友一样，还是当年的那个老郭与于谦，相谈甚欢。<br>&#160; &#160; &#160; &#160;到了大学城地铁站，我建议骑个小黄去学院吧，老郭说咱们走过去吧，我说好，这样也可以用脚步重新再走一遍这个地方。<strong>于是，我们就顶着今天的太阳，感受着校庆60周年的余热，北门的一句“欢迎校友回家”甚是暖心，是啊，才毕业不到三个月的我们，已然是这所大学的校友，感叹时间过得如此的飞快，老郭突然冒出一句：“下一次回到这里就不知道是什么时候了，或许在十年后吧！”我紧接着附和道：“是啊，可能是十年后了吧！”之所以与老郭的关系不断深入，就是因为知道他的每一句话背后的故事，以及他在想什么，而他所想的同时也是我想的，或许这就是我们能够说到一块的原因吧。此刻，他又在感叹，感叹曾经的故事！</strong>走在宽大的校园马路上，随处可见工大60周年校庆的牌子，还挺美的，与大家分享一下哈！<br><img src="/NextLegend.github.io/2018/09/09/这里的人与这里的故事/004.jpg" alt="这里的人与这里的故事"><br>&#160; &#160; &#160; &#160;走到这里的时候，突然有三四个中年阿姨，问我们：“同学，你知道校史馆怎么走吗？”我和老郭给这些校友前辈指了校史馆的位置，老郭说要不咱们也去看看吧，之前我没去过，我说好。就这样，我们作为年轻校友在前面给校友前辈带路，到了校史馆，我和老郭在前面观看学校的历史与珍贵的仪器，顺便听讲着学生讲解员给她们的讲解。<strong>在走到一台上了年纪的纺织仪器面前，老郭出于一贯的质疑与好奇思维，尝试着搞明白它的机械原理（被我偷拍了，哈哈）</strong><br><img src="/NextLegend.github.io/2018/09/09/这里的人与这里的故事/001.jpg" alt="这里的人与这里的故事"><br>&#160; &#160; &#160; &#160;<strong>还有这个—————&gt;</strong><br><img src="/NextLegend.github.io/2018/09/09/这里的人与这里的故事/002.jpg" alt="这里的人与这里的故事"><br>&#160; &#160; &#160; &#160;在经过时间里程计的时候，我突然握住老郭的大手，我说一起见证这伟大的时刻吧，而老郭突然配乐道：“当当当当。。。。。。”我禁不住笑了起来，你这是瓦格纳的《婚礼进行曲》啊，有点尴尬，哈哈。<br><img src="/NextLegend.github.io/2018/09/09/这里的人与这里的故事/003.jpg" alt="这里的人与这里的故事"><br>&#160; &#160; &#160; &#160;参观完校史馆，我们迫不及待的赶紧前往学院，先去了老师办公室，结果发现没人，可能是周末的原因吧。。。。。。。。然后我们就去考研自习室找了会煜大神，时间也十一点了，我们商量着要不先去吃饭去吧，就边走边聊，老郭和会煜大神谈起来更是津津有味，他们两更是同道中人。<strong>（这次回来，发现大家都没怎么变，还是老样子，真切、幽默、调侃、又互相关心着彼此的发展，或许这就是好朋友最真的面貌吧！）吃饭回来在学院一楼又聊了聊，聊到了过去，聊到了现在，还聊到了未来。</strong><br>&#160; &#160; &#160; &#160;聊到了大概十二点半左右，我和老郭看着时间也不早了，不能影响了会煜大神的节奏，我们就与他告别离开了，<strong>希望今年他能够考上自己心仪的学校，也是我们专业，甚至学院最有希望与能力的。其实，自己从他那里也学到了很多很多，做事态度认真，求真务实、追求完美、说话只说自己很有把握的话，给人一种非常踏实的感觉与印象，就是每一件事都交给他，让人很放心，而且他不仅会完成任务，而且还会给你优化与一些建议，这就是他，会煜大神，关于他的故事已然成为我们专业，乃至学院的神话，人人皆知，人人皆视其为榜样！</strong><br>&#160; &#160; &#160; &#160;再后来，我和老郭联系了一下老师，老师说他刚到办公室，我们去办公室找他，就这样，我和老郭准备了半个小时就去看望老师了。和老师谈了两三个小时，谈到了过去，谈到了现在，谈到了未来。谈到了学业、谈到了工作、谈到了个人理想。老郭又有些感触了，(我总觉得他有些不甘心，有些自卑)老师似乎也发现了，就鼓励他说其实做技术积累个两三年也挺好的，现在的研究生动手能力太差了，连最基本的仪器都不会使用，到时候找工作就不如你们这些已经工作了两三年的，只是他们起点比你们现在高罢了，老郭听后觉得也有道理，目光些许明亮起来，给老师说，他有这个自信能够在单位里做好。<strong>（以老郭的能力与思维能力，我相信三五年后，或许我该叫他李所或者李部长了。）后来又和老师聊了很多，老师也相应的给了一些建议，让我们不管在社会上还是学校里，都要实事求是，踏踏实实做技术，规划好自己的时间与人生，该来的总会来的，要懂得隐忍与坚守！！！</strong><br>&#160; &#160; &#160; &#160;四点左右，我和老郭看着时间不早了，也不想打扰老师工作（周末老师还来实验室，可见他的敬业与乐业精神所在）我们就和老师道别后，离开了。<br>&#160; &#160; &#160; &#160;<strong>最后想说，自己虽然现在已是一名研究生了，两年半后自己也面临着找工作，进入社会这个象牙塔，到时候是以怎样的姿态以及怎样的精神面貌迎接那时候的社会与工作，全在这不到三年里的每一天的进步与成长，就像老郭一样，思考着前行着前行着而又思考着，生活就是这样。人生路上能够遇到这样的恩师很难得，也很庆幸自己能够在求学路上遇见很多这样的恩师，古语云：“十年树木，百年树人；插柳之恩；终生难忘！”最后，明天是教师节，提前预祝天下的所有教师节日快乐！</strong></p><p><div align="right"> <strong>——2018年9月9日夜晚 于天津大学北洋园</strong></div></p>]]></content>
    
    <summary type="html">
    
      今天是2018年9月9日，明天是9月10日教师节，昨天小亮的大学同学兼比赛队长再兼相声老郭（哈哈，我给他取的，因为他说话自带天津的相声幽默风味）和小亮商量着今天一起去看望大学老师，今天重新回到这个待了四年，至今不愿去回忆的地方，再次坚定自己当初的选择与坚守！！！
    
    </summary>
    
      <category term="朋友 人生导师" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/%E6%9C%8B%E5%8F%8B-%E4%BA%BA%E7%94%9F%E5%AF%BC%E5%B8%88/"/>
    
    
      <category term="人生路上的朋友" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E4%BA%BA%E7%94%9F%E8%B7%AF%E4%B8%8A%E7%9A%84%E6%9C%8B%E5%8F%8B/"/>
    
  </entry>
  
  <entry>
    <title>计算图上的微积分：反向传播</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/09/05/%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8A%E7%9A%84%E5%BE%AE%E7%A7%AF%E5%88%86%EF%BC%9A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/</id>
    <published>2018-09-05T13:36:14.000Z</published>
    <updated>2018-09-05T13:46:36.296Z</updated>
    
    <content type="html"><![CDATA[<p><strong>一、介绍</strong><br>反向传播是一种关键的算法，它可以使训练深度模型在计算上易于处理。对于现代神经网络来说，相对于一个简单的实现，它可以使梯度下降的训练速度达到1000万倍。这就是一周训练和用20万年时间训练的模型之间的区别。<br>除了在深度学习中使用之外，反向传播在许多其他领域是一个强大的计算工具，从天气预报到分析数字稳定性，它只是在不同的领域用不同的名字。事实上，该算法在不同的领域至少被重新改造了几十次（见Griewank（2010））。一般，应用程序独立，名称是“反向模式区分”。<br>从根本上说，这是一种快速计算微分的技术。在你的包里，这是一个很重要的技巧，不仅在深度学习中，而且在各种各样的数字计算环境中。<br><strong>二、计算图</strong><br>计算图是一种思考数学表达式的好方法。例如，考虑表达式e =(a + b)∗(b + 1)。这里有三个操作：两个加法和一个乘法。为了帮助我们讨论这个问题，让我们引入两个中间变量，<br>c和d，所以每个函数的输出都有一个变量。我们现在有:<br>                            c=a + b<br>                            d=b + 1<br>                            e=c * d<br>为了创建一个计算图，我们将这些操作连同输入变量一起放入节点。当一个节点的值是另一个节点的输入时，一个箭头从一个节点到另一个节点。<br><img src="/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/001.png" alt="计算图上的微积分：反向传播"><br>在计算机科学中，这些图表一直都在出现，尤其是在谈论功能程序的时候。它们与依赖关系图和调用图的概念密切相关。它们也是流行的深度学习框架Theano背后的核心抽象。<br>我们可以通过将输入变量设置为特定的值和通过图表计算节点来评估表达式。例如,让我们设置a = 2和b = 1 :<br><img src="/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/002.png" alt="计算图上的微积分：反向传播"><br>表达式的求值结果为6。<br><strong>三、计算图的导数</strong><br>如果想要在计算图中理解导数，关键是要理解导数的边界。如果a直接影响c，然后我们想知道它是如何影响的c。如果a稍微改变一下，那c如何改变?我们称其c是关于a的偏导数。<br>为了求出这张图中的偏导数，我们需要求和规则和乘积法则：<br><img src="/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/003.png" alt="计算图上的微积分：反向传播"><br>下图中每条边都有导数。<br><img src="/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/004.png" alt="计算图上的微积分：反向传播"><br>如果我们想要了解没有直接连接的节点是如何相互影响的呢？让我们考虑一下e是如何被a影响的。如果我们以1的速度改变a，c同样的变化速度1改变。反过来, c以1的速度变化导致e以2的改变速率。所以e变化速率1∗2关于a。一般规则是对从一个节点到另一个节点的所有可能路径求和，将路径的每个边的导数相乘。例如，要得到e关于b的导数。我们得到:<br><img src="/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/005.png" alt="计算图上的微积分：反向传播"><br>这就解释了b是如何影响e到c的，以及它是如何通过d来影响它的。这种一般的“对路径求和”规则只是一种不同的关于多元链式法则的思考方式。<br><strong>四、因式分解路径</strong><br>仅仅“对路径求和”的问题是，在可能的路径中，很容易得到一个组合爆炸。<br><img src="/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/006.png" alt="计算图上的微积分：反向传播"><br>在上面的图中，从X到Y有三条路径，从Y到Z还有三条路径。如果我们想求导∂Z/∂X的话，通过对所有路径求和，我们需要求和3∗3 = 9条道路:<br><img src="/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/007.png" alt="计算图上的微积分：反向传播"><br>上面只有9条路径，但是当图形变得更加复杂时，很容易就会有成倍增长的路径。与其简单地对路径求和，不如把它们因式分解：<br><img src="/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/008.png" alt="计算图上的微积分：反向传播"><br>这就是“前向传播”和“反向传播”。它们是通过分解路径来有效计算总和的算法。它们不是显式地对所有路径求和，而是通过在每个节点上合并路径来更有效地计算相同的总和。事实上，这两种算法都能精确地触碰到每条边！ 前向传播从图的输入开始，然后向末端移动。在每个节点上，它都能计算出所有的路径。每条路径都代表了输入影响该节点的一种方式。通过把它们加起来，我们得到了节点受输入影响的总方式，它是导数。<br><img src="/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/009.png" alt="计算图上的微积分：反向传播"><br>虽然你可能没有从图的角度来考虑它，但是向前模式的微分和你在微积分课上做过的介绍是非常相似的。另一方面，反向传播的微分，从图的输出开始，向开始移动。在每个节点上，它合并了源自该节点的所有路径。<br><img src="/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/010.png" alt="计算图上的微积分：反向传播"><br>前向传播微分研究一个输入如何影响每个节点。反向传播微分研究每个节点如何影响一个输出。也就是说，正向模式微分应用算子∂/∂X对每个节点，反向模式微分应用算子∂Z/∂每一个节点。<br><strong>五、Computational Victories</strong><br>在这一点上，您可能想知道为什么有人会关心反向传播的微分。这看起来像是一种奇怪的方法，可以做与前模一样的事情。有什么好处吗？让我们再来看看我们最初的例子：<br><img src="/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/011.png" alt="计算图上的微积分：反向传播"><br>我们可以使用正向模式的微分b向上，这就给了我们每个结点的导数b。<br><img src="/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/012.png" alt="计算图上的微积分：反向传播"><br>我们计算∂e/∂b，这个导数是我们的输出对我们的输入的导数。如果我们做反向模式的微分从e开始? 这就得到了e对于每个节点的微分。<br><img src="/NextLegend.github.io/2018/09/05/计算图上的微积分：反向传播/013.png" alt="计算图上的微积分：反向传播"><br>当我说反向传播微分给我们对每个结点的导数时，我确实是指每个结点。我们得到两个∂e/∂a和∂e/∂b，e的导数是关于两个输入。前向传播的微分给了我们输出对单个输入的导数，但是反向模式的微分给了我们所有的结果。对于这个图，这只是两个因子的加速，但是想象一个有上百万个输入和一个输出的函数。前向传播的微分要求我们通过这个图上百万次来得到导数。反向传播的微分可以一下子把它们都弄到手！一个百万分之一的速度是相当不错的！在训练神经网络时，我们考虑的是成本（描述神经网络的糟糕程度）作为参数的函数（描述网络行为的数字）。我们想要计算所有参数的成本的导数，用于梯度下降。现在，在神经网络中，通常有数百万甚至数千万个参数。所以，反向模式的分化，在神经网络的背景下被称为反向传播，给我们一个巨大的速度！<br>（有任何情况下，正向模式的分化更有意义吗？是的,有! 当反向模式给出一个输出对所有输入的导数时，正向模式给出了所有输出对一个输入的导数。如果一个函数有大量的输出，那么正向模式的微分就会大大加快。)<br><strong>六、这不是简单的吗?</strong><br>Isn’t This Trivial?<br>当我第一次理解反向传播的时候，我的反应是：“哦，这就是链式法则！我们怎么花了这么长时间才弄明白？“我不是唯一一个有这种反应的人。”的确，如果你问“在前馈神经网络中有一种聪明的计算导数的方法吗？”“答案并不难。<br>但我认为这比表面上看起来要困难得多。你看，在反向传播发明的时候，人们并不是很关注我们研究的前馈神经网络。同样不明显的是，微分是培训它们的正确方式。一旦你意识到你可以快速计算出导数，这些就很明显了。这是一种循环依赖。<br>更糟糕的是，在不经意的想法中，把循环依赖的任何部分都写下来是很容易的。用微分工具训练神经网络？你肯定会被困在局部最优解里。很明显，计算所有这些导数都很昂贵。这只是因为我们知道这种方法是有效的，我们不会立即开始列出它可能不会的原因。<br>这是事后诸葛亮的好处。一旦你提出了这个问题，最困难的工作就已经完成了。<br><strong>七、结论</strong><br>微分比你想象的要便宜。这是我们从这篇文章中得到的主要教训。事实上，它们的价格并不便宜，而美国愚蠢的人不得不反复发现这一事实。在深度学习中，这是很重要的一点。在其他领域中，这也是一件非常有用的事情，只有当它不是常识的时候才会知道。还有其他的教训吗？我认为有。反向传播也是理解微分如何流经模型的有用工具。这对于解释为什么有些模型很难优化是非常有用的。最典型的例子是在重复的神经网络中消失的梯度问题。最后，我认为从这些技术中可以得到一个广泛的算法教训。反向传播和正向模式的区别使用强大的一对技巧（线性化和动态规划）来比人们想象的更有效地计算导数。如果您真正理解了这些技术，您可以使用它们来有效地计算其他涉及到微分的有趣表达式。我们将在以后的博客文章中探讨这个问题。这篇文章给出了一个非常抽象的反向传播的方法。我强烈建议阅读Michael Nielsen关于它的章节，进行精彩的讨论，更具体地关注神经网络。<br><strong>八、致谢</strong><br>感谢Greg Corrado，Jon Shlens，Samy Bengio和an利亚Angelova，感谢他们花时间校对这篇文章。也要感谢达里奥阿米迪、迈克尔尼尔森和约书亚本吉奥讨论解释反向传播的方法。也要感谢那些在演讲和研讨会系列中允许我练习解释反向传播的人！</p>]]></content>
    
    <summary type="html">
    
      小亮最近在看以色列大佬的NLP书籍《Neural Network Methods for Natural Language Processing》这里是第五章里面的内容中的推荐部分：关于微分的资料，详情请见下文！
    
    </summary>
    
      <category term="计算图 神经网络" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/%E8%AE%A1%E7%AE%97%E5%9B%BE-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="神经网络" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>再谈数据结构</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/09/05/%E5%86%8D%E8%B0%88%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/09/05/再谈数据结构/</id>
    <published>2018-09-05T12:22:02.000Z</published>
    <updated>2018-09-05T13:06:35.841Z</updated>
    
    <content type="html"><![CDATA[<p><strong>我把书籍PDF版本和配套代码放在我的百度云里，附上链接地址：</strong><br><strong>百度云链接：<a href="https://pan.baidu.com/s/1dp9K-KljcZoctUL37yCvWg" target="_blank" rel="noopener">https://pan.baidu.com/s/1dp9K-KljcZoctUL37yCvWg</a> 密码：43po</strong><br>小亮的同学（栗同学）在大学和小亮是一个专业的，研究生申请到了法国格勒诺布尔大学，（很优秀哈）。先介绍一下这个大学哈：<br>格勒诺布尔大学集团（格勒诺布尔-阿尔卑斯大学）是一所拥有近七百年历史的国立研究型大学，其科研实力处于法国顶尖水平，诞生过两位诺贝尔奖获得者（克劳斯·冯·克利青Klaus von Klitzing，路易·奈尔Louis Néel），一位图灵奖获得者（Joseph Sifakis），同时也是联合国教科文组织国际传播学教席（Chaire UNESCO）所在处。<br><strong>院校声誉：具有世界影响力的法国顶尖大学</strong><br>优势专业：自然科学、医学、社会与人文科学、语言学、信息传播学<br><strong>中国教育部是否认证：获得认证</strong><br>全球排名：<br><strong>CWUR（2018）世界大学排名第97位 </strong><br>USNews世界大学排名 （2018）全球大学排名第146位<br>ARWU （2017）世界大学学术排名第152位<br>THE（2018）泰晤士高等教育世界大学排名第301-350位<br>QS（2018）世界大学排名第236位<br>韦伯麦特里克斯网(Webometrics)世界大学（2018）排名第295名<br><strong>下面是这个学校的校园，是不是很美呐！！！</strong><br><img src="/NextLegend.github.io/2018/09/05/再谈数据结构/001.png" alt="再谈数据结构"><br><img src="/NextLegend.github.io/2018/09/05/再谈数据结构/002.jpg" alt="再谈数据结构"><br><img src="/NextLegend.github.io/2018/09/05/再谈数据结构/003.jpg" alt="再谈数据结构"><br><strong>好啦，言归正传！下面介绍：数据结构PDF与配套代码-C语言-严蔚敏</strong><br><img src="/NextLegend.github.io/2018/09/05/再谈数据结构/004.jpg" alt="再谈数据结构"><br><strong>小亮将大学上数据结构（C语言）这门课的课件和实验也单独整理出来了，放在一个文件夹中（数据结构PPT），分享给大家！（如果大家觉得不和胃口，可以忽略这个文件夹，直接跳转到下面的文件夹哈。）</strong><br><img src="/NextLegend.github.io/2018/09/05/再谈数据结构/005.png" alt="再谈数据结构"><br><strong>然后是《数据结构(C语言版).严蔚敏_吴伟民》扫描版PDF和本书的配套代码，如下图所示:</strong><br><img src="/NextLegend.github.io/2018/09/05/再谈数据结构/008.png" alt="再谈数据结构"><br><img src="/NextLegend.github.io/2018/09/05/再谈数据结构/006.png" alt="再谈数据结构"><br><img src="/NextLegend.github.io/2018/09/05/再谈数据结构/007.png" alt="再谈数据结构"><br><strong>一点点感受：其实，数据结构小亮觉得非常重要，他强调逻辑的严密性和算法的高效性。最近小亮的师兄也在找工作——&gt;机器学习方向算法岗，亲身感受到了算法基础的重要性，而算法又不是短期内能够提升的，贵在平时的积累与代码实践，所以小亮特意整理了这些资料，分享给技术小伙伴们！</strong></p>]]></content>
    
    <summary type="html">
    
      小亮今天和去法国读研的大学同学聊了聊，他需要数据结构C语言版本的资料，我就整理了一下，毕竟算法岗位很讲究逻辑的严谨性和规则，数据结构很重要的。详情请见下文！
    
    </summary>
    
      <category term="数据结构 C语言 算法" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-C%E8%AF%AD%E8%A8%80-%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据结构" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>汉字拼音转换工具_Python版</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/09/03/%E6%B1%89%E5%AD%97%E6%8B%BC%E9%9F%B3%E8%BD%AC%E6%8D%A2%E5%B7%A5%E5%85%B7-Python%E7%89%88/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/09/03/汉字拼音转换工具-Python版/</id>
    <published>2018-09-03T13:39:48.000Z</published>
    <updated>2018-09-04T03:01:48.198Z</updated>
    
    <content type="html"><![CDATA[<p>环境配置：<br>Wn10+CPU i7-6700<br>Pycharm 2018<br>python 3.6<br>numpy 1.14.5<br><strong>一、github介绍</strong><br><strong>先附上github的一张图片哈：</strong><br><img src="/NextLegend.github.io/2018/09/03/汉字拼音转换工具-Python版/001.jpg" alt="汉字拼音转换工具_Python版"><br><strong>github地址如下：</strong><br><strong><a href="https://github.com/mozillazg/python-pinyin" target="_blank" rel="noopener">https://github.com/mozillazg/python-pinyin</a></strong><br><strong>二、特性</strong><br>(1) 根据词组智能匹配最正确的拼音。<br>(2) 支持多音字。<br>(3) 简单的繁体支持, 注音支持。<br>(4) 支持多种不同拼音/注音风格。<br><strong>三、安装</strong><br><strong>注意：以下两种安装方式选择其一即可</strong><br>1、pip安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install pypinyin</span><br></pre></td></tr></table></figure></p><p>2、pycharm安装<br><img src="/NextLegend.github.io/2018/09/03/汉字拼音转换工具-Python版/003.png" alt=""><br><img src="/NextLegend.github.io/2018/09/03/汉字拼音转换工具-Python版/004.png" alt=""><br><strong>四、代码实践</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> pypinyin <span class="keyword">import</span> pinyin, lazy_pinyin, Style</span><br><span class="line"></span><br><span class="line">value1 = pinyin(<span class="string">'天津大学'</span>)</span><br><span class="line">print(value1)</span><br><span class="line"></span><br><span class="line">value2 = pinyin(<span class="string">'天津大学'</span>, heteronym=<span class="keyword">True</span>)  <span class="comment"># 启用多音字模式</span></span><br><span class="line">print(value2)</span><br><span class="line"></span><br><span class="line">value3 =  pinyin(<span class="string">'天津大学'</span>, style=Style.FIRST_LETTER)  <span class="comment"># 设置拼音风格</span></span><br><span class="line">print(value3)</span><br><span class="line"></span><br><span class="line">value4 = pinyin(<span class="string">'天津大学'</span>, style=Style.TONE2, heteronym=<span class="keyword">True</span>)</span><br><span class="line">print(value4)</span><br><span class="line"></span><br><span class="line">value5 = pinyin(<span class="string">'天津大学'</span>, style=Style.BOPOMOFO)  <span class="comment"># 注音风格</span></span><br><span class="line">print(value5)</span><br><span class="line"></span><br><span class="line">value6 = pinyin(<span class="string">'天津大学'</span>, style=Style.CYRILLIC)  <span class="comment"># 俄语字母风格</span></span><br><span class="line">print(value6)</span><br><span class="line"></span><br><span class="line">value7 = lazy_pinyin(<span class="string">'天津大学'</span>)  <span class="comment"># 不考虑多音字的情况</span></span><br><span class="line">print(value7)</span><br></pre></td></tr></table></figure></p><p><strong>五、实验结果</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[&apos;tiān&apos;], [&apos;jīn&apos;], [&apos;dà&apos;], [&apos;xué&apos;]]</span><br><span class="line">[[&apos;tiān&apos;], [&apos;jīn&apos;], [&apos;dà&apos;], [&apos;xué&apos;]]</span><br><span class="line">[[&apos;t&apos;], [&apos;j&apos;], [&apos;d&apos;], [&apos;x&apos;]]</span><br><span class="line">[[&apos;tia1n&apos;], [&apos;ji1n&apos;], [&apos;da4&apos;], [&apos;xue2&apos;]]</span><br><span class="line">[[&apos;ㄊㄧㄢ&apos;], [&apos;ㄐㄧㄣ&apos;], [&apos;ㄉㄚˋ&apos;], [&apos;ㄒㄩㄝˊ&apos;]]</span><br><span class="line">[[&apos;тянь1&apos;], [&apos;цзинь1&apos;], [&apos;да4&apos;], [&apos;сюэ2&apos;]]</span><br><span class="line">[&apos;tian&apos;, &apos;jin&apos;, &apos;da&apos;, &apos;xue&apos;]</span><br></pre></td></tr></table></figure></p><p><img src="/NextLegend.github.io/2018/09/03/汉字拼音转换工具-Python版/002.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      小亮今天看到一个不错的项目，现在和大家分享一下哈！该项目是基于hotoo/pinyin开发。将汉字转为拼音，可以用于汉字注音、排序、检索(Russian translation) 。详情请见下文！
    
    </summary>
    
      <category term="Python NLP" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/Python-NLP/"/>
    
    
      <category term="NLP" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>2018算法工程师秋招集锦</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/09/03/2018%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%A7%8B%E6%8B%9B%E9%9B%86%E9%94%A6/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/09/03/2018算法工程师秋招集锦/</id>
    <published>2018-09-03T11:38:35.000Z</published>
    <updated>2018-09-03T11:38:35.441Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Learning Pyspark</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/08/25/Learning-Pyspark/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/08/25/Learning-Pyspark/</id>
    <published>2018-08-25T02:37:09.000Z</published>
    <updated>2018-08-25T03:35:58.810Z</updated>
    
    <content type="html"><![CDATA[<p><strong>我把书籍PDF版本和配套代码放在我的百度云里，附上链接地址：</strong><br><strong>百度云链接：<a href="https://pan.baidu.com/s/1EogSZ3mT4tAYZyqj6WprIg" target="_blank" rel="noopener">https://pan.baidu.com/s/1EogSZ3mT4tAYZyqj6WprIg</a> 密码：vsmv</strong><br><strong>下面介绍一下这本书：Learning Pyspark</strong><br><img src="/NextLegend.github.io/2018/08/25/Learning-Pyspark/Learning_Pyspark.png" alt="Learning Pyspark"></p><p>感谢您选择本书开始您的PySpark冒险，我希望您像我一样兴奋。当DennyLee第一次告诉我这本新书的时候很高兴 - 使Apache Spark成为最重要的事情之一精彩的平台，支持Java / Scala / JVM世界和Python（以及最近的R）世界。许多以前针对Spark的书都是专注于所有核心语言，或主要关注JVM语言，所以很高兴看到PySpark有机会用这样的专用书来发光经验丰富的Spark教育家。通过支持这两个不同的世界，我们是能够更有效地作为数据科学家和数据工程师一起工作窃取彼此社区的最佳想法。能够有机会审查其早期版本是一种荣幸这本书，只是增加了我对这个项目的兴奋。我有这个特权参加一些相同的会议和聚会，并观看作者向各种受众介绍Spark世界的新概念（从第一部分开始）定时器到老手），他们做了很好的工作，提炼他们的经验这本书。作者的经验从他们的作品中汲取了一切对所涉及主题的解释。除了简单介绍PySpark之外，他们还有还花时间查看来自社区的新闻包，例如GraphFrames和TensorFrames。我认为社区是决定时经常被忽视的组件之一使用什么工具，Python有一个很棒的社区，我很期待您加入了PythonSpark社区。所以，享受你的冒险;我知道你是与Denny Lee和TomekDrabas保持良好关系。我真的相信这一点一个多样化的Spark用户社区，我们将能够制作更好的工具。大家好，所以我希望能在一次会议，聚会或邮寄中见到你很快列出:)霍尔顿卡劳<br>附：<br>我欠丹尼一杯啤酒;如果你想给我买一个Bud Light lime（或lime-a-rita）我非常感激（虽然他可能不像我那样有趣）.<br><strong>本书作者：Tomasz Drabas</strong><br>Tomasz Drabas是一名为微软工作的数据科学家，目前居住在微软西雅图地区。 他在数据分析和数据科学方面拥有超过13年的经验在众多的领域：先进技术，航空公司，电信，金融，他在三大洲工作时获得了咨询：欧洲，澳大利亚，和北美。 在澳大利亚期间，Tomasz一直在攻读他的博士学位运营研究，重点关注选择建模和收益管理航空业的应用。</p>]]></content>
    
    <summary type="html">
    
      该书籍为2017年由Tomasz Drabas出版的英文原版Learning Pyspark。书籍技术路线是：在本地构建数据密集型应用程序并部署大规模使用Python和Spark 2.0的组合功能。
    
    </summary>
    
      <category term="Spark" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/Spark/"/>
    
    
      <category term="大数据" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Python之禅</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/08/24/Python%E4%B9%8B%E7%A6%85/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/08/24/Python之禅/</id>
    <published>2018-08-24T13:47:44.000Z</published>
    <updated>2018-08-24T13:50:01.292Z</updated>
    
    <content type="html"><![CDATA[<p><strong>The Zen of Python, by Tim Peters</strong><br>Beautiful is better than ugly.<br>Explicit is better than implicit.<br>Simple is better than complex.<br>Complex is better than complicated.<br>Flat is better than nested.<br>Sparse is better than dense.<br>Readability counts.<br>Special cases aren’t special enough to break the rules.<br>Although practicality beats purity.<br>Errors should never pass silently.<br>Unless explicitly silenced.<br>In the face of ambiguity, refuse the temptation to guess.<br>There should be one– and preferably only one –obvious way to do it.<br>Although that way may not be obvious at first unless you’re Dutch.<br>Now is better than never.<br>Although never is often better than <em>right</em> now.<br>If the implementation is hard to explain, it’s a bad idea.<br>If the implementation is easy to explain, it may be a good idea.<br>Namespaces are one honking great idea – let’s do more of those!</p><p><strong>Tim Peters的Python之禅</strong><br>美丽胜过丑陋。<br>显式优于隐式。<br>简单比复杂更好。<br>复杂比复杂更好。<br>Flat优于嵌套。<br>稀疏优于密集。<br>可读性很重要。<br>特殊情况不足以打破规则。<br>虽然实用性胜过纯洁。<br>错误不应该默默地传递。<br>除非明确沉默。<br>面对模棱两可，拒绝猜测的诱惑。<br>应该有一个 - 最好只有一个 - 显而易见的方法。<br>虽然这种方式起初可能并不明显，除非你是荷兰人。<br>现在总比没有好。<br>虽然现在永远不会比*正确好。<br>如果实施很难解释，那是个坏主意。<br>如果实现很容易解释，那可能是个好主意。<br>命名空间是一个很棒的主意 - 让我们做更多的事情吧！</p>]]></content>
    
    <summary type="html">
    
      Tim Peters的Python之禅
    
    </summary>
    
      <category term="Python" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>GAN的理解与TensorFlow的实现</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/08/24/GAN%E7%9A%84%E7%90%86%E8%A7%A3%E4%B8%8ETensorFlow%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/08/24/GAN的理解与TensorFlow的实现/</id>
    <published>2018-08-24T07:32:48.000Z</published>
    <updated>2018-08-24T09:54:54.959Z</updated>
    
    <content type="html"><![CDATA[<p>笔者信息：Next_Legend  QQ:1219154092 人工智能 自然语言处理 图像处理 神经网络 高维信息处理<br>                                                                                                                                                                                                                                                                              ——2018.7.31于天津大学<br>一、前言<br>本文会从头介绍生成对抗式网络的一些内容，从生成式模型开始说起，到GAN的基本原理，InfoGAN，AC-GAN的基本科普，如果有任何有错误的地方，请随时喷，我刚开始研究GAN这块的内容，希望和大家一起学习。<br>二、生成式模型<br>何为生成式模型？在很多machine learning的教程或者公开课上，通常会把machine learning的算法分为两类： 生成式模型、判别式模型；其区别在于： 对于输入x，类别标签y，在生成式模型中估计其联合概率分布，而判别式模型估计其属于某类的条件概率分布。 常见的判别式模型包括：LogisticRegression， SVM, Neural Network等等，生成式模型包括：Naive Bayes， GMM， Bayesian Network， MRF 等等。<br>三、研究生成式模型的意义<br>生成式模型的特性主要包括以下几个方面：<br>    在应用数学和工程方面，生成式模型能够有效地表征高维数据分布；<br>    生成式模型能够作为一种技术手段辅助强化学习，能够有效表征强化学习模型中的state状态(这里不扩展，后面会跟RL的学习笔记)；<br>    对semi-supervised learning也有比较好的效果，能够在miss data下训练模型，并在miss data下给出相应地输出；<br>    在对于一个输入伴随多个输出的场景下，生成式模型也能够有效工作，而传统的机器学习方法通过最小化模型输出和期望输出的某个object function的值 无法训练单输入多输出的模型，而生成式模型，尤其是GAN能够hold住这种场景，一个典型的应用是通过场景预测video的下一帧。<br>生成式模型一些典型的应用：<br>    图像的超分辨率<br>    iGAN：Generative Visual Manipulation on the Natural Image Manifold<br>    图像转换<br>四、生成式模型族谱<br><img src="https://img-blog.csdn.net/20180804190740652?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br>上图涵盖了基本的生成式模型的方法，主要按是否需要定义概率密度函数分为：<br>Explicit density models<br>explicit density models 又分为tractable explicit models和逼近的explicit model，怎么理解呢，tractable explicit model通常可以直接通过数学方法来建模求解，而基于逼近的explicit model通常无法直接对数据分布进行建模，可以利用数学里的一些近似方法来做数据建模， 通常基于逼近的explicit model分为确定性（变分方法：如VAE的lower bound）和随机性的方法（马尔科夫链蒙特卡洛方法）。<br>    VAE lower bound：<br>    <img src="https://img-blog.csdn.net/20180804190904726?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br>    马尔科夫链蒙特卡洛方法（MCMC），一种经典的基于马尔科夫链的抽样方法，通过多次来拟合分布。比较好的教程：A Beginner’s Guide to Monte Carlo Markov Chain MCMC Analysis, An Introduction to MCMC for Machine Learning.<br>Implicit density models<br>无需定义明确的概率密度函数，代表方法包括马尔科夫链、生成对抗式网络（GAN），该系列方法无需定义数据分布的描述函数。</p><p>五、生成对抗式网络与其他生成式网络对比<br>生成对抗式网络（GAN）能够有效地解决很多生成式方法的缺点，主要包括：</p><ul><li>并行产生samples；</li><li>生成式函数的限制少，如无需合适马尔科夫采样的数据分布（Boltzmann machines），生成式函数无需可逆、latent code需与sample同维度（nonlinear ICA）；</li><li>无需马尔科夫链的方法（Boltzmann machines， GSNs）；</li><li>相对于VAE的方法，无需variational bound；<br>GAN比其他方法一般来说性能更好。</li><li>可以使用冒号来定义对齐方式：</li></ul><p>六、GAN工作原理<br>GAN主要由两部分构成：generator和discriminator，generator主要是从训练数据中产生相同分布的samples，而discriminator 则是判断输入是真实数据还是generator生成的数据，discriminator采用传统的监督学习的方法。这里我们可以这样类比，generator 是一个伪造假币的专业人士，discriminator是警察，generator的目的是制造出尽可能以假乱真的假钞，而discriminator是为了能 鉴别是否为假钞，最终整个gan会达到所谓的纳什均衡，Goodfellow在他的paperGAN的理解与TF的实现-小石头的码疯窝中有严格的数学证明，当$p_G$==$p_{data}$时达到 全局最优：<br><img src="https://img-blog.csdn.net/20180804191235601?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>另一个比较明显看得懂的图如下：<br><img src="https://img-blog.csdn.net/20180804191312769?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>图中黑色点线为真实数据分布$p_{data}$，绿色线为generator生成的数据分布$p_{G}$,而Discriminator就是蓝色点线，其目的是为了将$p_{data}$和$p_{G}$ 区分，(a)中是初始状态，然后会更新Discriminator中的参数，若干次step之后，Discriminator有了较大的判断力即到了(b)的状态，之后会更新G的模型使其生成的数据分布（绿色线）更加趋近与真实数据分布， 若干次G和D的模型参数更新后，理论上最终会达到(d)的状态即G能够产生和真实数据完全一致的分布(证明见上一张图)，如从随机数据分布生成人脸像。<br>七、如何训练GAN<br>因为GAN结构的不同，和常规训练一个dl model方法不同， 这里采用simultaneous SGD，每一个step中，会有两个两个梯度优化的 过程，一个是更新discriminator的参数来最小化$J_{(D)}$，一个是更新generator的参数来最小$J_{(G)}$，通常会选用Adam来作为最优化的优化器， 也有人建议可以不等次数地更新generator和discriminator（有相关工作提出，1：1的在实际中更有效：Adam: A Method for Stochastic Optimization） 如何训练GAN，在Goodfellow的GAN的tutorial还有一些代码中有更多的描述包括不同的cost function， 这里我就不详细展开了。<br>1、DCGAN<br>GAN出来后很多相关的应用和方法都是基于DCGAN的结构，DCGAN即”Deep Convolution GAN”，通常会有一些约定俗成的规则：<br><img src="https://img-blog.csdn.net/20180804191418817?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><ul><li>在Discriminator和generator中大部分层都使用batch normalization，而在最后一层时通常不会使用batch normalizaiton，目的 是为了保证模型能够学习到数据的正确的均值和方差；</li><li>因为会从random的分布生成图像，所以一般做需要增大图像的空间维度时如77-&gt;1414， 一般会使用strdie为2的deconv（transposed convolution）；</li><li>通常在DCGAN中会使用Adam优化算法而不是SGD。<br>2、各种GAN<br><img src="https://img-blog.csdn.net/2018080419152318?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>这里有个大神把各种gan的paper都做了一个统计AdversarialNetsPapers<br>这里大家有更多的兴趣可以直接去看对应的paper，我接下来会尽我所能描述下infogan和AC-GAN这两块的内容<br>3、InfoGAN<br>InfoGAN是一种能够学习disentangled representation的GAN，何为disentangled representation？比如人脸数据集中有各种不同的属性特点，如脸部表情、是否带眼睛、头发的风格眼珠的颜色等等，这些很明显的相关表示， InfoGAN能够在完全无监督信息（是否带眼睛等等）下能够学习出这些disentangled representation，而相对于传统的GAN，只需修改loss来最大化GAN的input的noise（部分fixed的子集）和最终输出之间的互信息。<br>4、原理<br>为了达到上面提到的效果，InfoGAN必须在input的noise来做一些文章，将noise vector划分为两部分：</li><li>z: 和原始的GAN input作用一致；</li><li>c: latent code，能够在之后表示数据分布中的disentangled representation<br>那么如何从latent code中学到相应的disentangled representation呢？ 在原始的GAN中，忽略了c这部分的影响，即GAN产生的数据分布满足$P_{G}(x|C)=P(x)$,为了保证能够利用c这部分信息， 作者提出这样一个假设：c与generator的输出相关程度应该很大，而在信息论中，两个数据分布的相关程度即互信息， 即generator的输出和input的c的$I(c;G(z,c))$应该会大。 所以，InfoGAN就变成如下的优化问题：<br><img src="https://img-blog.csdn.net/20180804191707262?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>因为互信息的计算需要后验概率的分布（下图红线部分），在实际中很难直接使用，因此，在实际训练中一般不会直接最大化$I(c;G(z,c))$<br><img src="https://img-blog.csdn.net/20180804191748702?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>这里作者采用和VAE类似的方法，增加一个辅助的数据分布为后验概率的low bound： 所以，这里互信息的计算如下：<br><img src="https://img-blog.csdn.net/20180804191813177?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>这里相关的证明就不深入了，有兴趣的可以去看看paper。<br>5、实验<br>我写的一版基于TensorFlow的Info-GAN实现：Info-GANburness/tensorflow-101 random的label信息，和对应生成的图像：<br><img src="https://img-blog.csdn.net/2018080419191142?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180804191920586?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>不同random变量控制产生同一class下的不同输出：<br><img src="https://img-blog.csdn.net/20180804191943725?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>6、AC-GAN<br>AC-GAN即auxiliary classifier GAN，对应的paper：[1610.09585] Conditional Image Synthesis With Auxiliary Classifier GANs, 如前面的示意图中所示，AC-GAN的Discriminator中会输出相应的class label的概率，然后更改loss fuction，增加class预测正确的概率， ac-gan是一个tensorflow相关的实现，基于作者自己开发的sugartensor，感觉和paper里面在loss函数的定义上差异，看源码的时候注意下，我这里有参考写了一个基于原生tensorflow的版本AC-GAN.<br>实验<br>各位有兴趣的可以拿代码在其他的数据集上也跑一跑，AC-GAN能够有效利用class label的信息，不仅可以在G时指定需要生成的image的label，同事该class label也能在Discriminator用来扩展loss函数，增加整个对抗网络的性能。 random的label信息，和对应生成的图像：<br><img src="https://img-blog.csdn.net/2018080419203481?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180804192043197?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>不同random变量控制产生同一class下的不同输出：<br><img src="https://img-blog.csdn.net/20180804192102877?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>七、总结<br>照例总结一下，本文中，我基本介绍了下生成式模型方法的各个族系派别，到GAN的基本内容，到InfoGAN、AC-GAN，大部分的工作都来自于阅读相关的paper，自己相关的工作就是 tensorflow下参考sugartensor的内容重现了InfoGAN、AC-GAN的相关内容。<br>当然，本人菜鸟一枚，难免有很多理解不到位的地方，写出来更多的是作为分享，让更多人了解GAN这块的内容，如果任何错误或不合适的地方，敬请在评论中指出，我们一起讨论一起学习 另外我的所有相关的代码都在github上:GAN,相信读一下无论是对TensorFlow的理解还是GAN的理解都会 有一些帮助，简单地参考mnist.py修改下可以很快的应用到你的数据集上，如果有小伙伴在其他数据集上做出有意思的实验效果的，欢迎分享。</li></ul><p>原文地址： <a href="http://www.leiphone.com/news/201702/GZsIbIb9V9AUGmb6.html" target="_blank" rel="noopener">http://www.leiphone.com/news/201702/GZsIbIb9V9AUGmb6.html</a></p>]]></content>
    
    <summary type="html">
    
      该教程前面主要讲解GAN网络的一些概念，后面会基于tensorflow实战，欢迎大家分享学习！
    
    </summary>
    
      <category term="Tensorflow实战深度学习" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/Tensorflow%E5%AE%9E%E6%88%98%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Tensorflow" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>基于深度学习tensorflow实现文本分类任务的注意力机制</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/08/24/Tensorflow%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8ERNN%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/08/24/Tensorflow实现基于RNN的文本分类任务的注意力机制/</id>
    <published>2018-08-24T07:23:48.000Z</published>
    <updated>2018-08-24T09:55:23.841Z</updated>
    
    <content type="html"><![CDATA[<p>要点：<br>该教程为深度学习tensorflow实现文本分类任务的注意力机制，实现可视化注意力文本。<br>环境配置：<br>Wn10+CPU i7-6700<br>Pycharm2018<br>Tensorflow 1.8.0<br>Tensorboard 1.8.0<br>笔者信息：Next_Legend  QQ:1219154092 人工智能 自然语言处理 图像处理 神经网络<br>                                                                                                 ——2018.8.8于天津大学</p><hr><p>一、下载代码<br>   该代码见笔者的资源下载部分<a href="https://download.csdn.net/download/jinyuan7708/10592063" target="_blank" rel="noopener">https://download.csdn.net/download/jinyuan7708/10592063</a><br>   代码不需要改动，只需要配置好环境和安装好相应的库，就可以训练和测试了。<br>二、相应的库文件<br>   tensorflow    1.8.0<br>   tensorboard  1.8.0<br>   numpy<br>   keras<br>   tqdm<br>三、工程目录文件<br>  <img src="https://img-blog.csdn.net/20180808222357233?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="工程目录文件"><br>  该项目主要包括attention.py       train.py     utils.py    visualize.py四个文件夹<br>  其中train.py文件是训练模型的文件，运行后会生成model.data-00000-of-00001、model.index、model.meta以及checkpoint文件，也就是训练生成的模型文件。<br>四、核心代码<br><img src="https://img-blog.csdn.net/20180808222935934?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br><img src="https://img-blog.csdn.net/20180808222945807?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br><img src="https://img-blog.csdn.net/20180808222954987?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br><strong>train.py文件代码</strong></p><php><pre><code>from __future__ import print_function, divisionimport numpy as npimport tensorflow as tffrom keras.datasets import imdbfrom tensorflow.contrib.rnn import GRUCellfrom tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnnfrom tqdm import tqdmfrom attention import attentionfrom utils import get_vocabulary_size, fit_in_vocabulary, zero_pad, batch_generatorNUM_WORDS = 10000INDEX_FROM = 3SEQUENCE_LENGTH = 250EMBEDDING_DIM = 100HIDDEN_SIZE = 150ATTENTION_SIZE = 50KEEP_PROB = 0.8BATCH_SIZE = 256NUM_EPOCHS = 3  # Model easily overfits without pre-trained words embeddings, that&apos;s why train for a few epochsDELTA = 0.5MODEL_PATH = &apos;./model&apos;# Load the data set(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)# Sequences pre-processingvocabulary_size = get_vocabulary_size(X_train)X_test = fit_in_vocabulary(X_test, vocabulary_size)X_train = zero_pad(X_train, SEQUENCE_LENGTH)X_test = zero_pad(X_test, SEQUENCE_LENGTH)# Different placeholderswith tf.name_scope(&apos;Inputs&apos;):batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name=&apos;batch_ph&apos;)target_ph = tf.placeholder(tf.float32, [None], name=&apos;target_ph&apos;)seq_len_ph = tf.placeholder(tf.int32, [None], name=&apos;seq_len_ph&apos;)keep_prob_ph = tf.placeholder(tf.float32, name=&apos;keep_prob_ph&apos;)# Embedding layerwith tf.name_scope(&apos;Embedding_layer&apos;):embeddings_var = tf.Variable(tf.random_uniform([vocabulary_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)tf.summary.histogram(&apos;embeddings_var&apos;, embeddings_var)batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)# (Bi-)RNN layer(-s)rnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),                    inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)tf.summary.histogram(&apos;RNN_outputs&apos;, rnn_outputs)# Attention layerwith tf.name_scope(&apos;Attention_layer&apos;):attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)tf.summary.histogram(&apos;alphas&apos;, alphas)# Dropoutdrop = tf.nn.dropout(attention_output, keep_prob_ph)# Fully connected layerwith tf.name_scope(&apos;Fully_connected_layer&apos;):W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE * 2, 1], stddev=0.1))  # Hidden size is multiplied by 2 for Bi-RNNb = tf.Variable(tf.constant(0., shape=[1]))y_hat = tf.nn.xw_plus_b(drop, W, b)y_hat = tf.squeeze(y_hat)tf.summary.histogram(&apos;W&apos;, W)with tf.name_scope(&apos;Metrics&apos;):    # Cross-entropy loss and optimizer initialization    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))    tf.summary.scalar(&apos;loss&apos;, loss) optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)# Accuracy metricaccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))tf.summary.scalar(&apos;accuracy&apos;, accuracy)merged = tf.summary.merge_all()train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE)train_writer = tf.summary.FileWriter(&apos;./logdir/train&apos;, accuracy.graph)test_writer = tf.summary.FileWriter(&apos;./logdir/test&apos;, accuracy.graph)session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))saver = tf.train.Saver()if __name__ == &quot;__main__&quot;:with tf.Session(config=session_conf) as sess:    sess.run(tf.global_variables_initializer())    print(&quot;Start learning...&quot;)    for epoch in range(NUM_EPOCHS):        loss_train = 0        loss_test = 0        accuracy_train = 0        accuracy_test = 0        print(&quot;epoch: {}\t&quot;.format(epoch), end=&quot;&quot;)        # Training        num_batches = X_train.shape[0] // BATCH_SIZE        for b in tqdm(range(num_batches)):            x_batch, y_batch = next(train_batch_generator)            seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences            loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer, merged],                                                feed_dict={batch_ph: x_batch,                                                           target_ph: y_batch,                                                           seq_len_ph: seq_len,                                                           keep_prob_ph: KEEP_PROB})            accuracy_train += acc            loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)            train_writer.add_summary(summary, b + num_batches * epoch)        accuracy_train /= num_batches        # Testing        num_batches = X_test.shape[0] // BATCH_SIZE        for b in tqdm(range(num_batches)):            x_batch, y_batch = next(test_batch_generator)            seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences            loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],                                                     feed_dict={batch_ph: x_batch,                                                                target_ph: y_batch,                                                                seq_len_ph: seq_len,                                                                keep_prob_ph: 1.0})            accuracy_test += acc            loss_test += loss_test_batch            test_writer.add_summary(summary, b + num_batches * epoch)        accuracy_test /= num_batches        loss_test /= num_batches        print(&quot;loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}&quot;.format(            loss_train, loss_test, accuracy_train, accuracy_test        ))    train_writer.close()    test_writer.close()    saver.save(sess, MODEL_PATH)    print(&quot;Run &apos;tensorboard --logdir=./logdir&apos; to checkout tensorboard logs.&quot;)</code></pre><p></p></php><br>五、训练过程<br>   <img src="https://img-blog.csdn.net/20180808224427275?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br>   笔者由于使用的 CPU来进行训练，所以速度比较慢，感兴趣的朋友可以考虑使用GPU来计算，可以大大减少训练模型的时间。如果不会搭建gpu环境的小伙伴可以参考我的另一篇Tensorflow gpu环境搭建 ，附上地址哈：<br>   <a href="https://blog.csdn.net/jinyuan7708/article/details/79642924" target="_blank" rel="noopener">https://blog.csdn.net/jinyuan7708/article/details/79642924</a><br>六、训练结果<br>  <img src="https://img-blog.csdn.net/20180808224902778?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="训练生成的model文件"><br>七、Tensorboard可视化<br><img src="https://img-blog.csdn.net/20180808225017120?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="accuracy"><br><img src="https://img-blog.csdn.net/20180808225026854?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="loss"><br><img src="https://img-blog.csdn.net/20180808225040674?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="计算图"><br>八、visualization可视化结果<br>得到模型后，再继续执行visualize.py文件，生成结果可视化。如下图：<br><img src="https://img-blog.csdn.net/20180808225248673?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br>至此，我们的教程就结束啦，代码等文件我上传到我的blog下载资源部分，欢迎大家下载批评指正哈!<br>代码地址：<a href="https://download.csdn.net/download/jinyuan7708/10592063" target="_blank" rel="noopener">https://download.csdn.net/download/jinyuan7708/10592063</a><p></p>]]></content>
    
    <summary type="html">
    
      该教程为深度学习tensorflow实现文本分类任务的注意力机制，实现可视化注意力文本。
    
    </summary>
    
      <category term="Tensorflow实战深度学习" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/Tensorflow%E5%AE%9E%E6%88%98%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Tensorflow" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>中英文NLP集成型工具汇总</title>
    <link href="https://legendtianjin.github.io/NextLegend.github.io/2018/08/24/%E4%B8%AD%E8%8B%B1%E6%96%87NLP%E9%9B%86%E6%88%90%E5%9E%8B%E5%B7%A5%E5%85%B7%E6%B1%87%E6%80%BB/"/>
    <id>https://legendtianjin.github.io/NextLegend.github.io/2018/08/24/中英文NLP集成型工具汇总/</id>
    <published>2018-08-24T07:22:48.000Z</published>
    <updated>2018-08-24T09:56:08.365Z</updated>
    
    <content type="html"><![CDATA[<p><strong>该文档简单总结了一下集成的中英文NLP工具，分享给NLP领域的大家！</strong><br>笔者信息：Next_Legend QQ:1219154092 机器学习 自然语言处理 计算机视觉 深度学习<br>——2018.8.19于天津大学</p><hr><p><strong>1、面向研究的StanfordNLP(Java) (CoreNLP/Parder/POS Tager/NER…) <a href="https://nlp.stanford.edu/software/index.shtml" target="_blank" rel="noopener">网页链接</a></strong><br><img src="https://img-blog.csdn.net/20180819205836357?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><strong>2、面向应用的SpaCy(Python) <a href="https://spacy.io/" target="_blank" rel="noopener">网页链接</a></strong><br><img src="https://img-blog.csdn.net/20180819205650448?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><strong>3、哈工大语言技术平台LTP(C++) <a href="https://github.com/HIT-SCIR/ltp" target="_blank" rel="noopener">网页链接</a></strong><br><img src="https://img-blog.csdn.net/20180819205956673?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><strong>4、本土的HanLP(Java) <a href="http://hanlp.hankcs.com/?sentence=%E6%88%91%E7%88%B1%E4%BD%A0%E4%B8%AD%E5%9B%BD%EF%BC%8C%E6%88%91%E5%9C%A8%E5%A4%A9%E6%B4%A5%E5%A4%A7%E5%AD%A6%EF%BC%8C%E6%88%91%E7%88%B1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%82" target="_blank" rel="noopener">网页链接</a></strong><br><img src="https://img-blog.csdn.net/2018081921031468?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><strong>5、轻量非主流的xmnlp(Python) <a href="https://github.com/SeanLee97/xmnlp" target="_blank" rel="noopener">网页链接</a></strong><br><img src="https://img-blog.csdn.net/20180819210436793?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><strong>6、相对零散的THUNLP开放项目 <a href="https://github.com/thunlp" target="_blank" rel="noopener">网页链接</a></strong><br><img src="https://img-blog.csdn.net/20180819210536217?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><strong>7、复旦大学NLP <a href="http://nlp.fudan.edu.cn/" target="_blank" rel="noopener">网页链接</a></strong><br><img src="https://img-blog.csdn.net/20180819210840388?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><strong>8、清华大学NLP <a href="http://thulac.thunlp.org/" target="_blank" rel="noopener">网页链接</a></strong><br><img src="https://img-blog.csdn.net/20180819211018822?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><strong>9、TEXTBLOG <a href="https://textblob.readthedocs.io/en/dev/" target="_blank" rel="noopener">网页链接</a></strong><br><img src="https://img-blog.csdn.net/20180819211200578?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><strong>10、PyNLPIR <a href="https://pypi.org/project/PyNLPIR/" target="_blank" rel="noopener">网页链接</a></strong><br><img src="https://img-blog.csdn.net/20180819211324571?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><strong>11、Polyglot <a href="https://polyglotclub.com/" target="_blank" rel="noopener">网页链接</a></strong><br><img src="https://img-blog.csdn.net/2018081921152564?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><strong>12、NLTK <a href="http://www.nltk.org/" target="_blank" rel="noopener">网页链接</a></strong><br><img src="https://img-blog.csdn.net/20180819211652437?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><strong>其他的NLP工具小编暂时没有了解，欢迎有使用经验的同学朋友补充！ </strong></p>]]></content>
    
    <summary type="html">
    
      该文档简单总结了一下集成的中英文NLP工具，分享给NLP领域的大家！
    
    </summary>
    
      <category term="NLP" scheme="https://legendtianjin.github.io/NextLegend.github.io/categories/NLP/"/>
    
    
      <category term="自然语言处理" scheme="https://legendtianjin.github.io/NextLegend.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
</feed>
